{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "colab": {
      "name": "In_class_exercise_04ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakhakhini/1/blob/main/In_class_exercise_04ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/29/2022)**"
      ],
      "metadata": {
        "id": "nh4BLSVc6185"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ],
      "metadata": {
        "id": "xPcYwXNm6189"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ],
      "metadata": {
        "tags": [],
        "id": "-SVSCj-n618-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oPdwNsd63ie",
        "outputId": "aead0a66-f479-41ae-b0ba-b952d9fc0240"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora, models, similarities, utils\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "class DirectoryCorpus(corpora.TextCorpus):\n",
        "\n",
        "    def get_texts(self):\n",
        "        length = 0\n",
        "        for root, dirs, files in os.walk(self.input):\n",
        "            for f in files:\n",
        "                try:\n",
        "                    content = ''\n",
        "                    with open(root + '/' + f, 'r') as content_file:\n",
        "                        content = utils.any2utf8(content_file.read())\n",
        "                    length += 1\n",
        "                    yield re.split(r'\\W+', content.lower())\n",
        "                except:\n",
        "                    pass\n",
        "        self.length = length\n",
        "\n",
        "corp_name = 'corpus-small'\n",
        "corp = DirectoryCorpus(corp_name)\n",
        "\n",
        "npasses = 50\n",
        "ntopics = 50\n",
        "nterms = 100\n",
        "model = models.LdaModel(corpus=corp, id2word=corp.dictionary, passes=npasses, num_topics=ntopics, distributed=False)\n",
        "model.save(corp_name + '.model')\n",
        "\n",
        "fh = open('topics.csv', 'w+')\n",
        "fh_csv = csv.writer(fh)\n",
        "fh_csv.writerow([('Term (%d)' if n % 2 == 0 else 'Score (%d)') % (n/2)  for n in range(2*ntopics)])\n",
        "for t in range(100):\n",
        "    fh_csv.writerow([(topics[n/2][t][1] if n % 2 == 0 else topics[n/2][t][0]) for n in range(2*ntopics)])"
      ],
      "metadata": {
        "id": "LvuD4_JR618_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ],
      "metadata": {
        "id": "EmPLCZiK619A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import Pipeline\n",
        "doc1=\"Mango and starberries are the sweet\"\n",
        "doc2=\"Science is her favorite subject\"\n",
        "doc3=\"Mathematics is an important subject\"\n",
        "doc4=\"Lemons and oranges are sour\"\n",
        "doc5=\"Algorithms and databases are tough\"\n",
        "documents = [doc1,doc2,doc3,doc4,doc5]\n",
        "documents\n",
        "vectorizer = TfidfVectorizer(stop_words='english', use_idf=True, smooth_idf=True)\n",
        "x = vectorizer.fit_transform(documents)\n",
        "#n_Components  = Number of Topics\n",
        "lsa = TruncatedSVD(n_components=2,n_iter=100)\n",
        "lsa.fit(x)\n",
        "terms = vectorizer.get_feature_names() \n",
        "print(lsa.components_)\n",
        "for ind,comp in enumerate(lsa.components_):\n",
        "    termsInComp = zip(terms,comp)\n",
        "    sortedTerms = sorted(termsInComp, key=lambda x: x[1],reverse=True)[:7]\n",
        "    print(\"Concept %d\" % ind)\n",
        "    for term in sortedTerms:\n",
        "        print(term[0])\n",
        "    print(\" \")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ZYcaGX619B",
        "outputId": "cef3d629-2d35-4907-8413-590f0f77d43f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.78028673e-16 -8.62482250e-16  3.89141461e-01  3.89141461e-01\n",
            "  -1.19327646e-15  1.01982687e-16  3.89141461e-01 -1.19327646e-15\n",
            "   3.89141461e-01 -1.19327646e-15  1.01982687e-16  6.27913762e-01\n",
            "   1.01982687e-16 -8.62482249e-16]\n",
            " [ 5.64863680e-01  5.64863680e-01  4.38362773e-16  2.25778168e-16\n",
            "  -1.19425110e-01  2.03273114e-17  2.25778168e-16 -1.19425110e-01\n",
            "   6.74303341e-16 -1.19425110e-01  2.03273114e-17  6.54743776e-16\n",
            "   2.03273114e-17  5.64863680e-01]]\n",
            "Concept 0\n",
            "subject\n",
            "favorite\n",
            "important\n",
            "mathematics\n",
            "science\n",
            "mango\n",
            "starberries\n",
            " \n",
            "Concept 1\n",
            "algorithms\n",
            "databases\n",
            "tough\n",
            "science\n",
            "subject\n",
            "favorite\n",
            "important\n",
            " \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ],
      "metadata": {
        "id": "V82jWOhy619C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Lda2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZj__czT6-6e",
        "outputId": "3a366a94-a30c-4132-a089-c7846c6e0ade"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Lda2vec\n",
            "  Downloading lda2vec-0.16.10.tar.gz (13 kB)\n",
            "Building wheels for collected packages: Lda2vec\n",
            "  Building wheel for Lda2vec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Lda2vec: filename=lda2vec-0.16.10-py3-none-any.whl size=14433 sha256=7adaa94cf68420e65492c08573d3c83071fb910fb01215d4fcd2c347c589fa32\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/51/1c/1e2960ded8d6e14b14454493fcc9c2d879a7fb99421867f177\n",
            "Successfully built Lda2vec\n",
            "Installing collected packages: Lda2vec\n",
            "Successfully installed Lda2vec-0.16.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "from lda2vec import EmbedMixture\n",
        "from lda2vec import dirichlet_likelihood\n",
        "from lda2vec.utils import move\n",
        "\n",
        "from chainer import Chain\n",
        "import chainer.links as L\n",
        "import chainer.functions as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LDA2Vec(Chain):\n",
        "    def __init__(self, n_documents=100, n_document_topics=10,\n",
        "                 n_units=256, n_vocab=1000, dropout_ratio=0.5, train=True,\n",
        "                 counts=None, n_samples=15, word_dropout_ratio=0.0,\n",
        "                 power=0.75, temperature=1.0):\n",
        "        em = EmbedMixture(n_documents, n_document_topics, n_units,\n",
        "                          dropout_ratio=dropout_ratio, temperature=temperature)\n",
        "        kwargs = {}\n",
        "        kwargs['mixture'] = em\n",
        "        kwargs['sampler'] = L.NegativeSampling(n_units, counts, n_samples,\n",
        "                                               power=power)\n",
        "        super(LDA2Vec, self).__init__(**kwargs)\n",
        "        rand = np.random.random(self.sampler.W.data.shape)\n",
        "        self.sampler.W.data[:, :] = rand[:, :]\n",
        "        self.n_units = n_units\n",
        "        self.train = train\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.word_dropout_ratio = word_dropout_ratio\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "    def prior(self):\n",
        "        dl1 = dirichlet_likelihood(self.mixture.weights)\n",
        "        return dl1\n",
        "\n",
        "    def fit_partial(self, rdoc_ids, rword_indices, window=5,\n",
        "                    update_only_docs=False):\n",
        "        doc_ids, word_indices = move(self.xp, rdoc_ids, rword_indices)\n",
        "        pivot_idx = next(move(self.xp, rword_indices[window: -window]))\n",
        "        pivot = F.embed_id(pivot_idx, self.sampler.W)\n",
        "        if update_only_docs:\n",
        "            pivot.unchain_backward()\n",
        "        doc_at_pivot = rdoc_ids[window: -window]\n",
        "        doc = self.mixture(next(move(self.xp, doc_at_pivot)),\n",
        "                           update_only_docs=update_only_docs)\n",
        "        loss = 0.0\n",
        "        start, end = window, rword_indices.shape[0] - window\n",
        "        context = (F.dropout(doc, self.dropout_ratio) +\n",
        "                   F.dropout(pivot, self.dropout_ratio))\n",
        "        for frame in range(-window, window + 1):\n",
        "            # Skip predicting the current pivot\n",
        "            if frame == 0:\n",
        "                continue\n",
        "            # Predict word given context and pivot word\n",
        "            # The target starts before the pivot\n",
        "            targetidx = rword_indices[start + frame: end + frame]\n",
        "            doc_at_target = rdoc_ids[start + frame: end + frame]\n",
        "            doc_is_same = doc_at_target == doc_at_pivot\n",
        "            rand = np.random.uniform(0, 1, doc_is_same.shape[0])\n",
        "            mask = (rand > self.word_dropout_ratio).astype('bool')\n",
        "            weight = np.logical_and(doc_is_same, mask).astype('int32')\n",
        "            # If weight is 1.0 then targetidx\n",
        "            # If weight is 0.0 then -1\n",
        "            targetidx = targetidx * weight + -1 * (1 - weight)\n",
        "            target, = move(self.xp, targetidx)\n",
        "            loss = self.sampler(context, target)\n",
        "            loss.backward()\n",
        "            if update_only_docs:\n",
        "                # Wipe out any gradient accumulation on word vectors\n",
        "                self.sampler.W.grad *= 0.0\n",
        "        return loss.data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kVZoFJPg619C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ],
      "metadata": {
        "id": "dHSkPRpp619D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "from bertopic._ctfidf import ClassTFIDF\n",
        "from bertopic._utils import MyLogger, check_documents_type, check_embeddings_shape, check_is_fitted\n",
        "from bertopic._mmr import mmr\n",
        "from bertopic.backend._utils import select_backend\n",
        "from bertopic import plotting\n",
        "class BERTopic:\n",
        "    def __init__(self,\n",
        "                 language: str = \"english\",\n",
        "                 top_n_words: int = 10,\n",
        "                 n_gram_range: Tuple[int, int] = (1, 1),\n",
        "                 min_topic_size: int = 10,\n",
        "                 nr_topics: Union[int, str] = None,\n",
        "                 low_memory: bool = False,\n",
        "                 calculate_probabilities: bool = False,\n",
        "                 diversity: float = None,\n",
        "                 seed_topic_list: List[List[str]] = None,\n",
        "                 embedding_model=None,\n",
        "                 umap_model: UMAP = None,\n",
        "                 hdbscan_model: hdbscan.HDBSCAN = None,\n",
        "                 vectorizer_model: CountVectorizer = None,\n",
        "                 verbose: bool = False,\n",
        "                 ):\n",
        "\n",
        "        # Topic-based parameters\n",
        "        if top_n_words > 30:\n",
        "            raise ValueError(\"top_n_words should be lower or equal to 30. The preferred value is 10.\")\n",
        "        self.top_n_words = top_n_words\n",
        "        self.min_topic_size = min_topic_size\n",
        "        self.nr_topics = nr_topics\n",
        "        self.low_memory = low_memory\n",
        "        self.calculate_probabilities = calculate_probabilities\n",
        "        self.diversity = diversity\n",
        "        self.verbose = verbose\n",
        "        self.seed_topic_list = seed_topic_list\n",
        "        self.topics = None\n",
        "        self.topic_mapper = None\n",
        "        self.topic_sizes = None\n",
        "        self.merged_topics = None\n",
        "        self.topic_embeddings = None\n",
        "        self.topic_sim_matrix = None\n",
        "        self.representative_docs = None\n",
        "\n",
        "    def fit(self,\n",
        "            documents: List[str],\n",
        "            embeddings: np.ndarray = None,\n",
        "            y: Union[List[int], np.ndarray] = None):\n",
        "        self.fit_transform(documents, embeddings, y)\n",
        "        return self\n",
        "\n",
        "    def fit_transform(self,\n",
        "                      documents: List[str],\n",
        "                      embeddings: np.ndarray = None,\n",
        "                      y: Union[List[int], np.ndarray] = None) -> Tuple[List[int],\n",
        "                                                                       Union[np.ndarray, None]]:\n",
        "        \n",
        "        check_documents_type(documents)\n",
        "        check_embeddings_shape(embeddings, documents)\n",
        "\n",
        "        documents = pd.DataFrame({\"Document\": documents,\n",
        "                                  \"ID\": range(len(documents)),\n",
        "                                  \"Topic\": None})\n",
        "\n",
        "        # Extract embeddings\n",
        "        if embeddings is None:\n",
        "            self.embedding_model = select_backend(self.embedding_model,\n",
        "                                                  language=self.language)\n",
        "            embeddings = self._extract_embeddings(documents.Document,\n",
        "                                                  method=\"document\",\n",
        "                                                  verbose=self.verbose)\n",
        "            logger.info(\"Transformed documents to Embeddings\")\n",
        "        else:\n",
        "            if self.embedding_model is not None:\n",
        "                self.embedding_model = select_backend(self.embedding_model,\n",
        "                                                      language=self.language)\n",
        "\n",
        "        # Reduce dimensionality with UMAP\n",
        "        if self.seed_topic_list is not None and self.embedding_model is not None:\n",
        "            y, embeddings = self._guided_topic_modeling(embeddings)\n",
        "        umap_embeddings = self._reduce_dimensionality(embeddings, y)\n",
        "\n",
        "        # Cluster UMAP embeddings with HDBSCAN\n",
        "        documents, probabilities = self._cluster_embeddings(umap_embeddings, documents)\n",
        "\n",
        "        # Sort and Map Topic IDs by their frequency\n",
        "        if not self.nr_topics:\n",
        "            documents = self._sort_mappings_by_frequency(documents)\n",
        "\n",
        "        # Extract topics by calculating c-TF-IDF\n",
        "        self._extract_topics(documents)\n",
        "\n",
        "        # Reduce topics\n",
        "        if self.nr_topics:\n",
        "            documents = self._reduce_topics(documents)\n",
        "\n",
        "        self._map_representative_docs(original_topics=True)\n",
        "        probabilities = self._map_probabilities(probabilities, original_topics=True)\n",
        "        predictions = documents.Topic.to_list()\n",
        "\n",
        "        return predictions, probabilities\n",
        "\n",
        "    def transform(self,\n",
        "                  documents: Union[str, List[str]],\n",
        "                  embeddings: np.ndarray = None) -> Tuple[List[int], np.ndarray]:\n",
        "        check_is_fitted(self)\n",
        "        check_embeddings_shape(embeddings, documents)\n",
        "\n",
        "        if isinstance(documents, str):\n",
        "            documents = [documents]\n",
        "\n",
        "        if embeddings is None:\n",
        "            embeddings = self._extract_embeddings(documents,\n",
        "                                                  method=\"document\",\n",
        "                                                  verbose=self.verbose)\n",
        "\n",
        "        umap_embeddings = self.umap_model.transform(embeddings)\n",
        "        logger.info(\"Reduced dimensionality with UMAP\")\n",
        "\n",
        "        predictions, probabilities = hdbscan.approximate_predict(self.hdbscan_model, umap_embeddings)\n",
        "        logger.info(\"Predicted clusters with HDBSCAN\")\n",
        "\n",
        "        if self.calculate_probabilities:\n",
        "            probabilities = hdbscan.membership_vector(self.hdbscan_model, umap_embeddings)\n",
        "            logger.info(\"Calculated probabilities with HDBSCAN\")\n",
        "        else:\n",
        "            probabilities = None\n",
        "\n",
        "        probabilities = self._map_probabilities(probabilities, original_topics=True)\n",
        "        predictions = self._map_predictions(predictions)\n",
        "        return predictions, probabilities\n",
        "\n",
        "    def topics_over_time(self,\n",
        "                         docs: List[str],\n",
        "                         topics: List[int],\n",
        "                         timestamps: Union[List[str],\n",
        "                                           List[int]],\n",
        "                         nr_bins: int = None,\n",
        "                         datetime_format: str = None,\n",
        "                         evolution_tuning: bool = True,\n",
        "                         global_tuning: bool = True) -> pd.DataFrame:\n",
        "        check_is_fitted(self)\n",
        "        check_documents_type(docs)\n",
        "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics, \"Timestamps\": timestamps})\n",
        "        global_c_tf_idf = normalize(self.c_tf_idf, axis=1, norm='l1', copy=False)\n",
        "\n",
        "        all_topics = sorted(list(documents.Topic.unique()))\n",
        "        all_topics_indices = {topic: index for index, topic in enumerate(all_topics)}\n",
        "\n",
        "        if isinstance(timestamps[0], str):\n",
        "            infer_datetime_format = True if not datetime_format else False\n",
        "            documents[\"Timestamps\"] = pd.to_datetime(documents[\"Timestamps\"],\n",
        "                                                     infer_datetime_format=infer_datetime_format,\n",
        "                                                     format=datetime_format)\n",
        "\n",
        "        if nr_bins:\n",
        "            documents[\"Bins\"] = pd.cut(documents.Timestamps, bins=nr_bins)\n",
        "            documents[\"Timestamps\"] = documents.apply(lambda row: row.Bins.left, 1)\n",
        "\n",
        "        # Sort documents in chronological order\n",
        "        documents = documents.sort_values(\"Timestamps\")\n",
        "        timestamps = documents.Timestamps.unique()\n",
        "        if len(timestamps) > 100:\n",
        "            warnings.warn(f\"There are more than 100 unique timestamps (i.e., {len(timestamps)}) \"\n",
        "                          \"which significantly slows down the application. Consider setting `nr_bins` \"\n",
        "                          \"to a value lower than 100 to speed up calculation. \")\n",
        "\n",
        "        # For each unique timestamp, create topic representations\n",
        "        topics_over_time = []\n",
        "        for index, timestamp in tqdm(enumerate(timestamps), disable=not self.verbose):\n",
        "\n",
        "            # Calculate c-TF-IDF representation for a specific timestamp\n",
        "            selection = documents.loc[documents.Timestamps == timestamp, :]\n",
        "            documents_per_topic = selection.groupby(['Topic'], as_index=False).agg({'Document': ' '.join,\n",
        "                                                                                    \"Timestamps\": \"count\"})\n",
        "            c_tf_idf, words = self._c_tf_idf(documents_per_topic, fit=False)\n",
        "\n",
        "            if global_tuning or evolution_tuning:\n",
        "                c_tf_idf = normalize(c_tf_idf, axis=1, norm='l1', copy=False)\n",
        "\n",
        "            # Fine-tune the c-TF-IDF matrix at timestamp t by averaging it with the c-TF-IDF\n",
        "            # matrix at timestamp t-1\n",
        "            if evolution_tuning and index != 0:\n",
        "                current_topics = sorted(list(documents_per_topic.Topic.values))\n",
        "                overlapping_topics = sorted(list(set(previous_topics).intersection(set(current_topics))))\n",
        "\n",
        "                current_overlap_idx = [current_topics.index(topic) for topic in overlapping_topics]\n",
        "                previous_overlap_idx = [previous_topics.index(topic) for topic in overlapping_topics]\n",
        "\n",
        "                c_tf_idf.tolil()[current_overlap_idx] = ((c_tf_idf[current_overlap_idx] +\n",
        "                                                          previous_c_tf_idf[previous_overlap_idx]) / 2.0).tolil()\n",
        "\n",
        "            # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation\n",
        "            # by simply taking the average of the two\n",
        "            if global_tuning:\n",
        "                selected_topics = [all_topics_indices[topic] for topic in documents_per_topic.Topic.values]\n",
        "                c_tf_idf = (global_c_tf_idf[selected_topics] + c_tf_idf) / 2.0\n",
        "\n",
        "            # Extract the words per topic\n",
        "            labels = sorted(list(documents_per_topic.Topic.unique()))\n",
        "            words_per_topic = self._extract_words_per_topic(words, c_tf_idf, labels)\n",
        "            topic_frequency = pd.Series(documents_per_topic.Timestamps.values,\n",
        "                                        index=documents_per_topic.Topic).to_dict()\n",
        "\n",
        "            # Fill dataframe with results\n",
        "            topics_at_timestamp = [(topic,\n",
        "                                    \", \".join([words[0] for words in values][:5]),\n",
        "                                    topic_frequency[topic],\n",
        "                                    timestamp) for topic, values in words_per_topic.items()]\n",
        "            topics_over_time.extend(topics_at_timestamp)\n",
        "\n",
        "            if evolution_tuning:\n",
        "                previous_topics = sorted(list(documents_per_topic.Topic.values))\n",
        "                previous_c_tf_idf = c_tf_idf.copy()\n",
        "\n",
        "        return pd.DataFrame(topics_over_time, columns=[\"Topic\", \"Words\", \"Frequency\", \"Timestamp\"])\n",
        "\n",
        "    def topics_per_class(self,\n",
        "                         docs: List[str],\n",
        "                         topics: List[int],\n",
        "                         classes: Union[List[int], List[str]],\n",
        "                         global_tuning: bool = True) -> pd.DataFrame:\n",
        "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics, \"Class\": classes})\n",
        "        global_c_tf_idf = normalize(self.c_tf_idf, axis=1, norm='l1', copy=False)\n",
        "\n",
        "        # For each unique timestamp, create topic representations\n",
        "        topics_per_class = []\n",
        "        for index, class_ in tqdm(enumerate(set(classes)), disable=not self.verbose):\n",
        "\n",
        "            # Calculate c-TF-IDF representation for a specific timestamp\n",
        "            selection = documents.loc[documents.Class == class_, :]\n",
        "            documents_per_topic = selection.groupby(['Topic'], as_index=False).agg({'Document': ' '.join,\n",
        "                                                                                    \"Class\": \"count\"})\n",
        "            c_tf_idf, words = self._c_tf_idf(documents_per_topic, fit=False)\n",
        "\n",
        "            # Fine-tune the timestamp c-TF-IDF representation based on the global c-TF-IDF representation\n",
        "            # by simply taking the average of the two\n",
        "            if global_tuning:\n",
        "                c_tf_idf = normalize(c_tf_idf, axis=1, norm='l1', copy=False)\n",
        "                c_tf_idf = (global_c_tf_idf[documents_per_topic.Topic.values + 1] + c_tf_idf) / 2.0\n",
        "\n",
        "            # Extract the words per topic\n",
        "            labels = sorted(list(documents_per_topic.Topic.unique()))\n",
        "            words_per_topic = self._extract_words_per_topic(words, c_tf_idf, labels)\n",
        "            topic_frequency = pd.Series(documents_per_topic.Class.values,\n",
        "                                        index=documents_per_topic.Topic).to_dict()\n",
        "\n",
        "            # Fill dataframe with results\n",
        "            topics_at_class = [(topic,\n",
        "                                \", \".join([words[0] for words in values][:5]),\n",
        "                                topic_frequency[topic],\n",
        "                                class_) for topic, values in words_per_topic.items()]\n",
        "            topics_per_class.extend(topics_at_class)\n",
        "\n",
        "        topics_per_class = pd.DataFrame(topics_per_class, columns=[\"Topic\", \"Words\", \"Frequency\", \"Class\"])\n",
        "\n",
        "        return topics_per_class\n",
        "\n",
        "    def find_topics(self,\n",
        "                    search_term: str,\n",
        "                    top_n: int = 5) -> Tuple[List[int], List[float]]:\n",
        "        if self.embedding_model is None:\n",
        "            raise Exception(\"This method can only be used if you did not use custom embeddings.\")\n",
        "\n",
        "        topic_list = list(self.topics.keys())\n",
        "        topic_list.sort()\n",
        "\n",
        "        # Extract search_term embeddings and compare with topic embeddings\n",
        "        search_embedding = self._extract_embeddings([search_term],\n",
        "                                                    method=\"word\",\n",
        "                                                    verbose=False).flatten()\n",
        "        sims = cosine_similarity(search_embedding.reshape(1, -1), self.topic_embeddings).flatten()\n",
        "\n",
        "        # Extract topics most similar to search_term\n",
        "        ids = np.argsort(sims)[-top_n:]\n",
        "        similarity = [sims[i] for i in ids][::-1]\n",
        "        similar_topics = [topic_list[index] for index in ids][::-1]\n",
        "\n",
        "        return similar_topics, similarity\n",
        "\n",
        "    def update_topics(self,\n",
        "                      docs: List[str],\n",
        "                      topics: List[int],\n",
        "                      n_gram_range: Tuple[int, int] = None,\n",
        "                      vectorizer_model: CountVectorizer = None):\n",
        "        check_is_fitted(self)\n",
        "        if not n_gram_range:\n",
        "            n_gram_range = self.n_gram_range\n",
        "\n",
        "        self.vectorizer_model = vectorizer_model or CountVectorizer(ngram_range=n_gram_range)\n",
        "\n",
        "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
        "        self._extract_topics(documents)\n",
        "\n",
        "    def get_topics(self) -> Mapping[str, Tuple[str, float]]:\n",
        "        check_is_fitted(self)\n",
        "        return self.topics\n",
        "\n",
        "    def get_topic(self, topic: int) -> Union[Mapping[str, Tuple[str, float]], bool]:\n",
        "        check_is_fitted(self)\n",
        "        if topic in self.topics:\n",
        "            return self.topics[topic]\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def get_topic_info(self, topic: int = None) -> pd.DataFrame:\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        info = pd.DataFrame(self.topic_sizes.items(), columns=['Topic', 'Count']).sort_values(\"Count\", ascending=False)\n",
        "        info[\"Name\"] = info.Topic.map(self.topic_names)\n",
        "\n",
        "        if topic:\n",
        "            info = info.loc[info.Topic == topic, :]\n",
        "\n",
        "        return info\n",
        "\n",
        "    def get_topic_freq(self, topic: int = None) -> Union[pd.DataFrame, int]:\n",
        "        check_is_fitted(self)\n",
        "        if isinstance(topic, int):\n",
        "            return self.topic_sizes[topic]\n",
        "        else:\n",
        "            return pd.DataFrame(self.topic_sizes.items(), columns=['Topic', 'Count']).sort_values(\"Count\",\n",
        "                                                                                                  ascending=False)\n",
        "\n",
        "    def get_representative_docs(self, topic: int = None) -> List[str]:\n",
        "        check_is_fitted(self)\n",
        "        if isinstance(topic, int):\n",
        "            return self.representative_docs[topic]\n",
        "        else:\n",
        "            return self.representative_docs\n",
        "\n",
        "    def reduce_topics(self,\n",
        "                      docs: List[str],\n",
        "                      topics: List[int],\n",
        "                      probabilities: np.ndarray = None,\n",
        "                      nr_topics: int = 20) -> Tuple[List[int], np.ndarray]:\n",
        "        check_is_fitted(self)\n",
        "        self.nr_topics = nr_topics\n",
        "        documents = pd.DataFrame({\"Document\": docs, \"Topic\": topics})\n",
        "\n",
        "        # Reduce number of topics\n",
        "        documents = self._reduce_topics(documents)\n",
        "        self.merged_topics = None\n",
        "        self._map_representative_docs()\n",
        "\n",
        "        # Extract topics and map probabilities\n",
        "        new_topics = documents.Topic.to_list()\n",
        "        new_probabilities = self._map_probabilities(probabilities)\n",
        "\n",
        "        return new_topics, new_probabilities\n",
        "\n",
        "    def save(self,\n",
        "             path: str,\n",
        "             save_embedding_model: bool = True) -> None:\n",
        "        with open(path, 'wb') as file:\n",
        "            if not save_embedding_model:\n",
        "                embedding_model = self.embedding_model\n",
        "                self.embedding_model = None\n",
        "                joblib.dump(self, file)\n",
        "                self.embedding_model = embedding_model\n",
        "            else:\n",
        "                joblib.dump(self, file)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls,\n",
        "             path: str,\n",
        "             embedding_model=None):\n",
        "        with open(path, 'rb') as file:\n",
        "            if embedding_model:\n",
        "                topic_model = joblib.load(file)\n",
        "                topic_model.embedding_model = select_backend(embedding_model)\n",
        "            else:\n",
        "                topic_model = joblib.load(file)\n",
        "            return topic_model\n",
        "\n",
        "    def get_params(self, deep: bool = False) -> Mapping[str, Any]:\n",
        "        out = dict()\n",
        "        for key in self._get_param_names():\n",
        "            value = getattr(self, key)\n",
        "            if deep and hasattr(value, 'get_params'):\n",
        "                deep_items = value.get_params().items()\n",
        "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
        "            out[key] = value\n",
        "        return out\n",
        "\n",
        "    def _extract_embeddings(self,\n",
        "                            documents: Union[List[str], str],\n",
        "                            method: str = \"document\",\n",
        "                            verbose: bool = None) -> np.ndarray:\n",
        "        if isinstance(documents, str):\n",
        "            documents = [documents]\n",
        "\n",
        "        if method == \"word\":\n",
        "            embeddings = self.embedding_model.embed_words(documents, verbose)\n",
        "        elif method == \"document\":\n",
        "            embeddings = self.embedding_model.embed_documents(documents, verbose)\n",
        "        else:\n",
        "            raise ValueError(\"Wrong method for extracting document/word embeddings. \"\n",
        "                             \"Either choose 'word' or 'document' as the method. \")\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def _map_predictions(self, predictions: List[int]) -> List[int]:\n",
        "        \"\"\" Map predictions to the correct topics if topics were reduced \"\"\"\n",
        "        mappings = self.topic_mapper.get_mappings(original_topics=True)\n",
        "        mapped_predictions = [mappings[prediction]\n",
        "                              if prediction in mappings\n",
        "                              else -1\n",
        "                              for prediction in predictions]\n",
        "        return mapped_predictions\n",
        "\n",
        "    def _reduce_dimensionality(self,\n",
        "                               embeddings: Union[np.ndarray, csr_matrix],\n",
        "                               y: Union[List[int], np.ndarray] = None) -> np.ndarray:\n",
        "        if isinstance(embeddings, csr_matrix):\n",
        "            self.umap_model = UMAP(n_neighbors=15,\n",
        "                                   n_components=5,\n",
        "                                   metric='hellinger',\n",
        "                                   low_memory=self.low_memory).fit(embeddings, y=y)\n",
        "        else:\n",
        "            self.umap_model.fit(embeddings, y=y)\n",
        "        umap_embeddings = self.umap_model.transform(embeddings)\n",
        "        logger.info(\"Reduced dimensionality with UMAP\")\n",
        "        return np.nan_to_num(umap_embeddings)\n",
        "\n",
        "    def _cluster_embeddings(self,\n",
        "                            umap_embeddings: np.ndarray,\n",
        "                            documents: pd.DataFrame) -> Tuple[pd.DataFrame,\n",
        "                                                              np.ndarray]\n",
        "        self.hdbscan_model.fit(umap_embeddings)\n",
        "        documents['Topic'] = self.hdbscan_model.labels_\n",
        "        probabilities = self.hdbscan_model.probabilities_\n",
        "\n",
        "        if self.calculate_probabilities:\n",
        "            probabilities = hdbscan.all_points_membership_vectors(self.hdbscan_model)\n",
        "\n",
        "        self._update_topic_size(documents)\n",
        "        self._save_representative_docs(documents)\n",
        "        self.topic_mapper = TopicMapper(self.hdbscan_model)\n",
        "        logger.info(\"Clustered UMAP embeddings with HDBSCAN\")\n",
        "        return documents, probabilities\n",
        "\n",
        "    def _guided_topic_modeling(self, embeddings: np.ndarray) -> Tuple[List[int], np.array]:\n",
        "        # Create embeddings from the seeded topics\n",
        "        seed_topic_list = [\" \".join(seed_topic) for seed_topic in self.seed_topic_list]\n",
        "        seed_topic_embeddings = self._extract_embeddings(seed_topic_list, verbose=self.verbose)\n",
        "        seed_topic_embeddings = np.vstack([seed_topic_embeddings, embeddings.mean(axis=0)])\n",
        "\n",
        "        # Label documents that are most similar to one of the seeded topics\n",
        "        sim_matrix = cosine_similarity(embeddings, seed_topic_embeddings)\n",
        "        y = [np.argmax(sim_matrix[index]) for index in range(sim_matrix.shape[0])]\n",
        "        y = [val if val != len(seed_topic_list) else -1 for val in y]\n",
        "\n",
        "        # Average the document embeddings related to the seeded topics with the\n",
        "        # embedding of the seeded topic to force the documents in a cluster\n",
        "        for seed_topic in range(len(seed_topic_list)):\n",
        "            indices = [index for index, topic in enumerate(y) if topic == seed_topic]\n",
        "            embeddings[indices] = np.average([embeddings[indices], seed_topic_embeddings[seed_topic]], weights=[3, 1])\n",
        "        return y, embeddings\n",
        "\n",
        "    def _extract_topics(self, documents: pd.DataFrame):\n",
        "        documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
        "        self.c_tf_idf, words = self._c_tf_idf(documents_per_topic)\n",
        "        self.topics = self._extract_words_per_topic(words)\n",
        "        self._create_topic_vectors()\n",
        "        self.topic_names = {key: f\"{key}_\" + \"_\".join([word[0] for word in values[:4]])\n",
        "                            for key, values in\n",
        "                            self.topics.items()}\n",
        "\n",
        "    def _save_representative_docs(self, documents: pd.DataFrame):\n",
        "        # Prepare the condensed tree and luf clusters beneath a given cluster\n",
        "        condensed_tree = self.hdbscan_model.condensed_tree_\n",
        "        raw_tree = condensed_tree._raw_tree\n",
        "        clusters = sorted(condensed_tree._select_clusters())\n",
        "        cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
        "\n",
        "        #  Find the points with maximum lambda value in each leaf\n",
        "        representative_docs = {}\n",
        "        for topic in documents['Topic'].unique():\n",
        "            if topic != -1:\n",
        "                leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, clusters[topic])\n",
        "\n",
        "                result = np.array([])\n",
        "                for leaf in leaves:\n",
        "                    max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
        "                    points = raw_tree['child'][(raw_tree['parent'] == leaf) & (raw_tree['lambda_val'] == max_lambda)]\n",
        "                    result = np.hstack((result, points))\n",
        "\n",
        "                representative_docs[topic] = list(np.random.choice(result, 3, replace=False).astype(int))\n",
        "\n",
        "        # Convert indices to documents\n",
        "        self.representative_docs = {topic: [documents.iloc[doc_id].Document for doc_id in doc_ids]\n",
        "                                    for topic, doc_ids in\n",
        "                                    representative_docs.items()}\n",
        "\n",
        "    def _map_representative_docs(self, original_topics: bool = False):\n",
        "        mappings = self.topic_mapper.get_mappings(original_topics)\n",
        "        representative_docs = self.representative_docs.copy()\n",
        "\n",
        "        # Update the representative documents\n",
        "        updated_representative_docs = {mappings[old_topic]: []\n",
        "                                       for old_topic, _ in representative_docs.items()}\n",
        "        for old_topic, docs in representative_docs.items():\n",
        "            new_topic = mappings[old_topic]\n",
        "            updated_representative_docs[new_topic].extend(docs)\n",
        "\n",
        "        self.representative_docs = updated_representative_docs\n",
        "\n",
        "    def _create_topic_vectors(self):\n",
        "        if self.embedding_model is not None:\n",
        "            topic_list = list(self.topics.keys())\n",
        "            topic_list.sort()\n",
        "            n = self.top_n_words\n",
        "\n",
        "            # Extract embeddings for all words in all topics\n",
        "            topic_words = [self.get_topic(topic) for topic in topic_list]\n",
        "            topic_words = [word[0] for topic in topic_words for word in topic]\n",
        "            embeddings = self._extract_embeddings(topic_words,\n",
        "                                                  method=\"word\",\n",
        "                                                  verbose=False)\n",
        "\n",
        "            topic_embeddings = []\n",
        "            for i, topic in enumerate(topic_list):\n",
        "                word_importance = [val[1] for val in self.get_topic(topic)]\n",
        "                if sum(word_importance) == 0:\n",
        "                    word_importance = [1 for _ in range(len(self.get_topic(topic)))]\n",
        "                topic_embedding = np.average(embeddings[i * n: n + (i * n)], weights=word_importance, axis=0)\n",
        "                topic_embeddings.append(topic_embedding)\n",
        "\n",
        "            self.topic_embeddings = topic_embeddings\n",
        "\n",
        "    def _c_tf_idf(self, documents_per_topic: pd.DataFrame, fit: bool = True) -> Tuple[csr_matrix, List[str]]:\n",
        "        documents = self._preprocess_text(documents_per_topic.Document.values)\n",
        "\n",
        "        if fit:\n",
        "            self.vectorizer_model.fit(documents)\n",
        "\n",
        "        words = self.vectorizer_model.get_feature_names()\n",
        "        X = self.vectorizer_model.transform(documents)\n",
        "\n",
        "        if self.seed_topic_list:\n",
        "            seed_topic_list = [seed for seeds in self.seed_topic_list for seed in seeds]\n",
        "            multiplier = np.array([1.2 if word in seed_topic_list else 1 for word in words])\n",
        "        else:\n",
        "            multiplier = None\n",
        "\n",
        "        if fit:\n",
        "            self.transformer = ClassTFIDF().fit(X, multiplier=multiplier)\n",
        "\n",
        "        c_tf_idf = self.transformer.transform(X)\n",
        "\n",
        "        self.topic_sim_matrix = cosine_similarity(c_tf_idf)\n",
        "\n",
        "        return c_tf_idf, words\n",
        "\n",
        "    def _update_topic_size(self, documents: pd.DataFrame):\n",
        "        sizes = documents.groupby(['Topic']).count().sort_values(\"Document\", ascending=False).reset_index()\n",
        "        self.topic_sizes = dict(zip(sizes.Topic, sizes.Document))\n",
        "\n",
        "    def _extract_words_per_topic(self,\n",
        "                                 words: List[str],\n",
        "                                 c_tf_idf: csr_matrix = None,\n",
        "                                 labels: List[int] = None) -> Mapping[str,\n",
        "                                                                      List[Tuple[str, float]]]:\n",
        "        if c_tf_idf is None:\n",
        "            c_tf_idf = self.c_tf_idf\n",
        "\n",
        "        if labels is None:\n",
        "            labels = sorted(list(self.topic_sizes.keys()))\n",
        "\n",
        "        # Get the top 30 indices and values per row in a sparse c-TF-IDF matrix\n",
        "        indices = self._top_n_idx_sparse(c_tf_idf, 30)\n",
        "        scores = self._top_n_values_sparse(c_tf_idf, indices)\n",
        "        sorted_indices = np.argsort(scores, 1)\n",
        "        indices = np.take_along_axis(indices, sorted_indices, axis=1)\n",
        "        scores = np.take_along_axis(scores, sorted_indices, axis=1)\n",
        "\n",
        "        # Get top 30 words per topic based on c-TF-IDF score\n",
        "        topics = {label: [(words[word_index], score)\n",
        "                          if word_index and score > 0\n",
        "                          else (\"\", 0.00001)\n",
        "                          for word_index, score in zip(indices[index][::-1], scores[index][::-1])\n",
        "                          ]\n",
        "                  for index, label in enumerate(labels)}\n",
        "\n",
        "        if self.diversity is not None:\n",
        "            if self.embedding_model is not None:\n",
        "\n",
        "                for topic, topic_words in topics.items():\n",
        "                    words = [word[0] for word in topic_words]\n",
        "                    word_embeddings = self._extract_embeddings(words,\n",
        "                                                            method=\"word\",\n",
        "                                                            verbose=False)\n",
        "                    topic_embedding = self._extract_embeddings(\" \".join(words),\n",
        "                                                            method=\"word\",\n",
        "                                                            verbose=False).reshape(1, -1)\n",
        "                    topic_words = mmr(topic_embedding, word_embeddings, words,\n",
        "                                    top_n=self.top_n_words, diversity=self.diversity)\n",
        "                    topics[topic] = [(word, value) for word, value in topics[topic] if word in topic_words]\n",
        "        topics = {label: values[:self.top_n_words] for label, values in topics.items()}\n",
        "\n",
        "        return topics\n",
        "\n",
        "    def _reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:\n",
        "        initial_nr_topics = len(self.get_topics())\n",
        "\n",
        "        if isinstance(self.nr_topics, int):\n",
        "            if self.nr_topics < initial_nr_topics:\n",
        "                documents = self._reduce_to_n_topics(documents)\n",
        "        elif isinstance(self.nr_topics, str):\n",
        "            documents = self._auto_reduce_topics(documents)\n",
        "        else:\n",
        "            raise ValueError(\"nr_topics needs to be an int or 'auto'! \")\n",
        "\n",
        "        logger.info(f\"Reduced number of topics from {initial_nr_topics} to {len(self.get_topic_freq())}\")\n",
        "        return documents\n",
        "\n",
        "    def _reduce_to_n_topics(self, documents: pd.DataFrame) -> pd.DataFrame:\n",
        "        # Track which topics where originally merged\n",
        "        if not self.merged_topics:\n",
        "            self.merged_topics = []\n",
        "\n",
        "        # Create topic similarity matrix\n",
        "        similarities = cosine_similarity(self.c_tf_idf)\n",
        "        np.fill_diagonal(similarities, 0)\n",
        "\n",
        "        # Find most similar topic to least common topic\n",
        "        topics = documents.Topic.tolist().copy()\n",
        "        mapped_topics = {}\n",
        "        while len(self.get_topic_freq()) > self.nr_topics + 1:\n",
        "            topic_to_merge = self.get_topic_freq().iloc[-1].Topic\n",
        "            topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
        "            similarities[:, topic_to_merge + 1] = -1\n",
        "            self.merged_topics.append(topic_to_merge)\n",
        "\n",
        "            # Update Topic labels\n",
        "            documents.loc[documents.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
        "            mapped_topics[topic_to_merge] = topic_to_merge_into\n",
        "            self._update_topic_size(documents)\n",
        "\n",
        "        # Map topics\n",
        "        mapped_topics = {from_topic: to_topic for from_topic, to_topic in zip(topics, documents.Topic.tolist())}\n",
        "        self.topic_mapper.add_mappings(mapped_topics)\n",
        "\n",
        "        # Update representations\n",
        "        documents = self._sort_mappings_by_frequency(documents)\n",
        "        self._extract_topics(documents)\n",
        "        self._update_topic_size(documents)\n",
        "        return documents\n",
        "\n",
        "    def _auto_reduce_topics(self, documents: pd.DataFrame) -> pd.DataFrame:\n",
        "        topics = documents.Topic.tolist().copy()\n",
        "        unique_topics = sorted(list(documents.Topic.unique()))[1:]\n",
        "        max_topic = unique_topics[-1]\n",
        "\n",
        "        # Find similar topics\n",
        "        if self.topic_embeddings is not None:\n",
        "            embeddings = np.array(self.topic_embeddings)\n",
        "        else:\n",
        "            embeddings = self.c_tf_idf.toarray()\n",
        "        norm_data = normalize(embeddings, norm='l2')\n",
        "        predictions = hdbscan.HDBSCAN(min_cluster_size=2,\n",
        "                                      metric='euclidean',\n",
        "                                      cluster_selection_method='eom',\n",
        "                                      prediction_data=True).fit_predict(norm_data[1:])\n",
        "\n",
        "        # Map similar topics\n",
        "        mapped_topics = {unique_topics[index]: prediction + max_topic\n",
        "                         for index, prediction in enumerate(predictions)\n",
        "                         if prediction != -1}\n",
        "        documents.Topic = documents.Topic.map(mapped_topics).fillna(documents.Topic).astype(int)\n",
        "        mapped_topics = {from_topic: to_topic for from_topic, to_topic in zip(topics, documents.Topic.tolist())}\n",
        "\n",
        "        # Update documents and topics\n",
        "        self.topic_mapper.add_mappings(mapped_topics)\n",
        "        documents = self._sort_mappings_by_frequency(documents)\n",
        "        self._extract_topics(documents)\n",
        "        self._update_topic_size(documents)\n",
        "        return documents\n",
        "\n",
        "    def _sort_mappings_by_frequency(self, documents: pd.DataFrame) -> pd.DataFrame:\n",
        "        self._update_topic_size(documents)\n",
        "\n",
        "        # Map topics based on frequency\n",
        "        df = pd.DataFrame(self.topic_sizes.items(), columns=[\"Old_Topic\", \"Size\"]).sort_values(\"Size\", ascending=False)\n",
        "        df = df[df.Old_Topic != -1]\n",
        "        sorted_topics = {**{-1: -1}, **dict(zip(df.Old_Topic, range(len(df))))}\n",
        "        self.topic_mapper.add_mappings(sorted_topics)\n",
        "\n",
        "        # Map documents\n",
        "        documents.Topic = documents.Topic.map(sorted_topics).fillna(documents.Topic).astype(int)\n",
        "        self._update_topic_size(documents)\n",
        "        return documents\n",
        "\n",
        "    def _map_probabilities(self,\n",
        "                           probabilities: Union[np.ndarray, None],\n",
        "                           original_topics: bool = False) -> Union[np.ndarray, None]:\n",
        "        mappings = self.topic_mapper.get_mappings(original_topics)\n",
        "\n",
        "        # Map array of probabilities (probability for assigned topic per document)\n",
        "        if probabilities is not None:\n",
        "            if len(probabilities.shape) == 2 and self.get_topic(-1):\n",
        "                mapped_probabilities = np.zeros((probabilities.shape[0],\n",
        "                                                 len(set(mappings.values())) - 1))\n",
        "                for from_topic, to_topic in mappings.items():\n",
        "                    if to_topic != -1 and from_topic != -1:\n",
        "                        mapped_probabilities[:, to_topic] += probabilities[:, from_topic]\n",
        "\n",
        "                return mapped_probabilities\n",
        "\n",
        "        return probabilities\n",
        "\n",
        "    def _preprocess_text(self, documents: np.ndarray) -> List[str]:\n",
        "        cleaned_documents = [doc.lower() for doc in documents]\n",
        "        cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in cleaned_documents]\n",
        "        cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n",
        "        if self.language == \"english\":\n",
        "            cleaned_documents = [re.sub(r'[^A-Za-z0-9 ]+', '', doc) for doc in cleaned_documents]\n",
        "        cleaned_documents = [doc if doc != \"\" else \"emptydoc\" for doc in cleaned_documents]\n",
        "        return cleaned_documents\n",
        "\n",
        "    @staticmethod\n",
        "    def _top_n_idx_sparse(matrix: csr_matrix, n: int) -> np.ndarray:\n",
        "        indices = []\n",
        "        for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n",
        "            n_row_pick = min(n, ri - le)\n",
        "            values = matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]]\n",
        "            values = [values[index] if len(values) >= index + 1 else None for index in range(n)]\n",
        "            indices.append(values)\n",
        "        return np.array(indices)\n",
        "\n",
        "    @staticmethod\n",
        "    def _top_n_values_sparse(matrix: csr_matrix, indices: np.ndarray) -> np.ndarray:\n",
        "        top_values = []\n",
        "        for row, values in enumerate(indices):\n",
        "            scores = np.array([matrix[row, value] if value is not None else 0 for value in values])\n",
        "            top_values.append(scores)\n",
        "        return np.array(top_values)\n",
        "\n",
        "    @classmethod\n",
        "    def _get_param_names(cls):\n",
        "        init_signature = inspect.signature(cls.__init__)\n",
        "        parameters = sorted([p.name for p in init_signature.parameters.values()\n",
        "                             if p.name != 'self' and p.kind != p.VAR_KEYWORD])\n",
        "        return parameters\n",
        "\n",
        "    def __str__(self):\n",
        "        parameters = \"\"\n",
        "        for parameter, value in self.get_params().items():\n",
        "            value = str(value)\n",
        "            if \"(\" in value and value[0] != \"(\":\n",
        "                value = value.split(\"(\")[0] + \"(...)\"\n",
        "            parameters += f\"{parameter}={value}, \"\n",
        "\n",
        "        return f\"BERTopic({parameters[:-2]})\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "syfrPEVB619E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ],
      "metadata": {
        "id": "S9ndVgpe619I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\n",
        "LDA is most preffered in this Case\n",
        "All the four algorithms have same input which is Bag of words in matrix format. \n",
        "LSA focus on reducing matrix dimension while LDA solves topic modeling problems.\n",
        "The challenge with BERT is that we are hard to determine the optimal number of dimension.\n",
        "In general, low dimension consume less resource but we may not able to distinguish opposite \n",
        "meaning words while high dimension overcome it but consuming more resource. \n",
        "Furthermore unlike LDA the other algorithm are not best suited when dealing with large dataset\n",
        "and are only effective for small dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jdRMqxR1619K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}