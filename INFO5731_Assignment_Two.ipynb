{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakhakhini/1/blob/main/INFO5731_Assignment_Two.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Two**\n",
        "\n",
        "In this assignment, you will try to gather text data from open data source via web scraping or API. After that you need to clean the text data and syntactic analysis of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "epRNoUfmOzks"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U nltk \n",
        "!pip install -U spacy\n",
        "!pip install benepar\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEvgeW9sOzk0",
        "outputId": "44258bb9-4853-49d5-be8e-a50b2d7fb115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/benepar_en3.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import benepar\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import requests\n",
        "import spacy\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "benepar.download(\"benepar_en3\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(40 points). Write a python program to collect text data from **either of the following sources** and save the data into a **csv file**:\n",
        "\n",
        "(1) Collect all the customer reviews of the product [Apple iPhone 11](https://www.amazon.com/Apple-iPhone-11-64GB-Unlocked/dp/B07ZPKF8RG/ref=sr_1_13?dchild=1&keywords=iphone+12&qid=1631721363&sr=8-13) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of the film [Shang-Chi and the Legend of the Ten Rings](https://www.imdb.com/title/tt9376612/reviews?ref_=tt_sa_3) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 100 most popular software from [G2](https://www.g2.com/) or [Capterra](https://www.capterra.com/)\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query [natural language processing](https://citeseerx.ist.psu.edu/search?q=natural+language+processing&submit.x=0&submit.y=0&sort=rlv&t=doc) from CiteSeerX.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the [Densho Digital Repository](https://ddr.densho.org/narrators/).\n",
        "\n",
        "(6) Collect the top 10000 tweets by using hashtag [\"#blacklivesmatter\"](https://twitter.com/hashtag/blacklivesmatter) from Twitter. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PuFPKhC0m1fd"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://www.imdb.com/title/tt9376612/reviews\"\n",
        "FIRST_PAGE_URL = BASE_URL + \"?ref_=tt_urv\"\n",
        "AJAX_PAGE_URL = BASE_URL + \"/_ajax\"\n",
        "\n",
        "\n",
        "def fetch_soup(url: str, session: requests.Session,\n",
        "               params: Dict[str, Any] = None) -> BeautifulSoup:\n",
        "    response = session.get(url, params=params)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    return soup\n",
        "\n",
        "\n",
        "def extract_content(soup: BeautifulSoup, query: Dict[str, Any]) -> List[str]:\n",
        "    items = soup.find_all(query['name'], query['attrs'])\n",
        "    content = [item.text.strip() for item in items]\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "def extract_reviews(soup: BeautifulSoup):\n",
        "    titles = extract_content(soup, {'name': 'a', 'attrs': {'class': 'title'}})\n",
        "    contents = extract_content(soup, {\n",
        "        'name': 'div',\n",
        "        'attrs': {\n",
        "            'class': 'text show-more__control'\n",
        "        }\n",
        "    })\n",
        "\n",
        "    return zip(titles, contents)\n",
        "\n",
        "\n",
        "def create_next_request_params(soup: BeautifulSoup) -> Dict[str, Any]:\n",
        "    pagination_div = soup.find('div', {'data-key': True})\n",
        "    return {'ref_': 'tt_ql_urv', 'paginationKey': pagination_div['data-key']}\n",
        "\n",
        "\n",
        "def fetch_reviews(pages: int = 10):\n",
        "    # Holds all the extracted reviews\n",
        "    reviews = []\n",
        "    # Create a session for the requests\n",
        "    session = requests.Session()\n",
        "    # Fetch contents of the first page\n",
        "    soup = fetch_soup(FIRST_PAGE_URL, session)\n",
        "    reviews.extend(extract_reviews(soup))\n",
        "    # Fetch contents of the first page\n",
        "    for _ in range(pages):\n",
        "        params = create_next_request_params(soup)\n",
        "        soup = fetch_soup(AJAX_PAGE_URL, session, params)\n",
        "        reviews.extend(extract_reviews(soup))\n",
        "\n",
        "    return reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eh2JK2EsOzlE"
      },
      "outputs": [],
      "source": [
        "reviews = fetch_reviews(80)\n",
        "raw_reviews_df = pd.DataFrame(reviews, columns=['title', 'review'])\n",
        "raw_reviews_df.to_csv('raw-reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Write a python program to **clean the text data** you collected above and save the data in a new column in the csv file. The data cleaning steps include:\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the [stopwords list](https://gist.github.com/sebleier/554280).\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming. \n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Load the downloaded reviews\n",
        "reviews_df = pd.read_csv('raw-reviews.csv')\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    chars_no_punct = [char for char in text if char not in string.punctuation]\n",
        "    text_no_punct = \"\".join(chars_no_punct)\n",
        "\n",
        "    return text_no_punct\n",
        "\n",
        "\n",
        "def remove_numbers(text: str) -> str:\n",
        "    return re.sub(r\"[0-9]\", \"\", text)\n",
        "\n",
        "\n",
        "def remove_stopwords(text: str) -> str:\n",
        "    words_no_stopwords = [\n",
        "        word for word in text.split() if word not in stopwords.words(\"english\")\n",
        "    ]\n",
        "    text_no_stopwords = \" \".join(words_no_stopwords)\n",
        "\n",
        "    return text_no_stopwords\n",
        "\n",
        "\n",
        "def apply_stemmer(text: str) -> str:\n",
        "    words_stemmed = [stemmer.stem(word) for word in text.split()]\n",
        "    text_stemmed = \" \".join(words_stemmed)\n",
        "\n",
        "    return text_stemmed\n",
        "\n",
        "\n",
        "def apply_lemmatizer(text: str) -> str:\n",
        "    words_lemmatized = [lemmatizer.lemmatize(word) for word in text.split()]\n",
        "    text_lemmatized = \" \".join(words_lemmatized)\n",
        "\n",
        "    return text_lemmatized\n",
        "\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    normalized_text = text.lower()\n",
        "    text_no_punct = remove_punctuation(normalized_text)\n",
        "    text_no_numbers = remove_numbers(text_no_punct)\n",
        "    text_no_stopwords = remove_stopwords(text_no_numbers)\n",
        "    # Stemmer has wierd cases where it destroys the word people -> peopl\n",
        "    # thus it is off by default\n",
        "    # text_stemmed = apply_stemmer(text_no_stopwords)\n",
        "    text_lemmatized = apply_lemmatizer(text_no_stopwords)\n",
        "\n",
        "    return text_lemmatized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XCzt9udTOzlN"
      },
      "outputs": [],
      "source": [
        "raw_reviews_df = pd.read_csv('raw-reviews.csv')\n",
        "clean_reviews_df = raw_reviews_df[['title', 'review']].applymap(clean_text)\n",
        "clean_reviews_df.to_csv('clean-reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(30 points). Write a python program to conduct **syntax and structure analysis** of the clean text you just saved above. The syntax and structure analysis includes: \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "zViXDzDcOzlR"
      },
      "outputs": [],
      "source": [
        "clean_reviews_df = pd.read_csv('clean-reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVS2JNUPOzlS"
      },
      "source": [
        "## Parts Of Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQKnPjPDHJHr",
        "outputId": "8b2e73e2-287c-4f44-f215-10efbb0898fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ': 29832, 'ADV': 11461, 'NOUN': 57267, 'VERB': 23968}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "PARTS_OF_SPEECH = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]\n",
        "\n",
        "\n",
        "def tag_pos(text: str) -> List[Tuple[str, str]]:\n",
        "    word_tokens = nltk.word_tokenize(text)\n",
        "    pos = nltk.pos_tag(word_tokens, tagset=\"universal\")\n",
        "\n",
        "    return pos\n",
        "\n",
        "\n",
        "def count_pos(tagged_words: List[Tuple[str, str]], pos: List[str]) -> Dict[str, int]:\n",
        "    counts = Counter(tag for _, tag in tagged_words)\n",
        "    # Filter out only the required pos\n",
        "    counts = {tag: counts[tag] for tag in pos}\n",
        "\n",
        "    return counts\n",
        "\n",
        "\n",
        "def total_pos_counts(pos_counts: List[Dict[str, int]],\n",
        "                     pos: List[str]) -> Dict[str, int]:\n",
        "    total_counts = {}\n",
        "    for pos_count in pos_counts:\n",
        "        for part in pos:\n",
        "            if part in total_counts:\n",
        "                total_counts[part] = total_counts[part] + pos_count[part]\n",
        "            else:\n",
        "                total_counts[part] = pos_count[part]\n",
        "\n",
        "    return total_counts\n",
        "\n",
        "\n",
        "pos_counts = [\n",
        "    count_pos(tag_pos(text), PARTS_OF_SPEECH)\n",
        "    for text in clean_reviews_df['review'].to_list()\n",
        "]\n",
        "# Sum pos counts of all sentences\n",
        "total_pos_counts(pos_counts, PARTS_OF_SPEECH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N8NYI2UOzlU"
      },
      "source": [
        "## Constituency Parsing and Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8x-whkxOzlU",
        "outputId": "ee1fab53-eead-43de-96f3-0a0c66aa672a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/distributions/distribution.py:46: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  'with `validate_args=False` to turn off validation.')\n"
          ]
        }
      ],
      "source": [
        "MAX_LENGTH = 200 \n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
        "\n",
        "\n",
        "def parse_dependencies(docs: List):\n",
        "    def make_relationship(doc):\n",
        "        return [(token, token.dep_, token.head) for token in doc]\n",
        "\n",
        "    relationships = [make_relationship(doc) for doc in docs]\n",
        "\n",
        "    return relationships\n",
        "\n",
        "\n",
        "def parse_constituents(docs: List):\n",
        "    tree = [list(doc.sents)[0]._.parse_string for doc in docs]\n",
        "\n",
        "    return tree\n",
        "\n",
        "def trim_text(text: str, max_length: int) -> str:\n",
        "    words = text.split()[:max_length]\n",
        "    text = \" \".join(words)\n",
        "\n",
        "    return text\n",
        "\n",
        "docs = [nlp(trim_text(review, MAX_LENGTH)) for review in clean_reviews_df['review'].to_list()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQadpANNOzlV"
      },
      "source": [
        "### Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpaxk9_9OzlW",
        "outputId": "e4d45124-8436-4ddd-c600-84c62a691f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependency Parsing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(big, 'amod', fan),\n",
              "  (fan, 'nsubj', saw),\n",
              "  (many, 'amod', reason),\n",
              "  (many, 'amod', reason),\n",
              "  (marvel, 'amod', reason),\n",
              "  (film, 'compound', reason),\n",
              "  (reason, 'nsubj', saw),\n",
              "  (saw, 'ROOT', saw),\n",
              "  (one, 'nummod', daughter),\n",
              "  (oldest, 'amod', daughter),\n",
              "  (daughter, 'nsubj', insisted),\n",
              "  (insisted, 'ccomp', saw),\n",
              "  (watch, 'xcomp', insisted),\n",
              "  (itand, 'nmod', movie),\n",
              "  (overall, 'amod', movie),\n",
              "  (impressed, 'amod', movie),\n",
              "  (movie, 'dobj', watch),\n",
              "  (though, 'mark', think),\n",
              "  (think, 'advcl', saw),\n",
              "  (enjoyable, 'amod', itselfi),\n",
              "  (insane, 'amod', candy),\n",
              "  (eye, 'compound', candy),\n",
              "  (candy, 'compound', story),\n",
              "  (story, 'compound', itselfi),\n",
              "  (itselfi, 'nsubj', talk),\n",
              "  (could, 'aux', talk),\n",
              "  (talk, 'ccomp', think),\n",
              "  (plot, 'npadvmod', noticed),\n",
              "  (noticed, 'amod', say),\n",
              "  (review, 'compound', ill),\n",
              "  (film, 'compound', ill),\n",
              "  (ill, 'compound', say),\n",
              "  (say, 'dobj', talk),\n",
              "  (looked, 'ccomp', think),\n",
              "  (great, 'acomp', looked),\n",
              "  (would, 'aux', seen),\n",
              "  (great, 'advmod', seen),\n",
              "  (seen, 'ccomp', think),\n",
              "  (big, 'amod', screen),\n",
              "  (screen, 'compound', story),\n",
              "  (story, 'dobj', seen),\n",
              "  (though, 'mark', was),\n",
              "  (enjoyable, 'nsubj', was),\n",
              "  (was, 'advcl', seen),\n",
              "  (nt, 'neg', was),\n",
              "  (one, 'attr', was),\n",
              "  (particularly, 'advmod', grabbed),\n",
              "  (grabbed, 'acl', one),\n",
              "  (impacted, 'ccomp', think),\n",
              "  (mepossibly, 'advmod', violent),\n",
              "  (violent, 'amod', consequence),\n",
              "  (frenetic, 'amod', consequence),\n",
              "  (film, 'compound', consequence),\n",
              "  (consequence, 'nsubj', lacked),\n",
              "  (lacked, 'ccomp', think),\n",
              "  (intimacy, 'compound', humanity),\n",
              "  (humanity, 'dobj', lacked)],\n",
              " [(okay, 'intj', son),\n",
              "  (movie, 'compound', son),\n",
              "  (simu, 'compound', liu),\n",
              "  (liu, 'compound', son),\n",
              "  (son, 'nsubj', see),\n",
              "  (seemingly, 'advmod', immortal),\n",
              "  (immortal, 'amod', master),\n",
              "  (master, 'appos', son),\n",
              "  (ten, 'nummod', ring),\n",
              "  (ring, 'appos', son),\n",
              "  (always, 'advmod', good),\n",
              "  (good, 'amod', son),\n",
              "  (see, 'ROOT', see),\n",
              "  (tony, 'compound', leung),\n",
              "  (leung, 'compound', hope),\n",
              "  (hope, 'nsubj', soaked),\n",
              "  (soaked, 'ccomp', see),\n",
              "  (disney, 'compound', wind),\n",
              "  (wind, 'dobj', soaked),\n",
              "  (opposing, 'acl', wind),\n",
              "  (dad, 'compound', slacker),\n",
              "  (awkwafina, 'compound', slacker),\n",
              "  (slacker, 'nmod', girlfriend),\n",
              "  (sortof, 'compound', girlfriend),\n",
              "  (girlfriend, 'compound', sister),\n",
              "  (fine, 'amod', lead),\n",
              "  (lead, 'compound', menger),\n",
              "  (menger, 'compound', sister),\n",
              "  (zhang, 'compound', sister),\n",
              "  (sister, 'dobj', opposing),\n",
              "  (michelle, 'compound', aunt),\n",
              "  (yeoh, 'compound', aunt),\n",
              "  (aunt, 'dobj', soaked),\n",
              "  (always, 'advmod', good),\n",
              "  (good, 'advmod', soaked),\n",
              "  (see, 'conj', soaked),\n",
              "  (hope, 'npadvmod', soaked),\n",
              "  (soaked, 'amod', disney),\n",
              "  (disney, 'compound', tooi),\n",
              "  (tooi, 'nsubj', grown),\n",
              "  (grown, 'ccomp', see),\n",
              "  (increasingly, 'advmod', fond),\n",
              "  (fond, 'amod', film),\n",
              "  (martial, 'amod', art),\n",
              "  (art, 'compound', film),\n",
              "  (film, 'dobj', grown),\n",
              "  (last, 'amod', couple),\n",
              "  (couple, 'compound', decade),\n",
              "  (decade, 'compound', decline),\n",
              "  (decline, 'compound', reason),\n",
              "  (movie, 'nmod', reason),\n",
              "  (musical, 'amod', reason),\n",
              "  (reason, 'nsubj', make),\n",
              "  (do, 'aux', make),\n",
              "  (nt, 'neg', make),\n",
              "  (make, 'conj', grown),\n",
              "  (movie, 'dobj', make),\n",
              "  (like, 'prep', make),\n",
              "  (try, 'pcomp', like),\n",
              "  (they, 'nsubj', embarrassing),\n",
              "  (re, 'aux', embarrassing),\n",
              "  (often, 'advmod', embarrassing),\n",
              "  (embarrassing, 'ccomp', try),\n",
              "  (people, 'nsubj', like),\n",
              "  (do, 'aux', like),\n",
              "  (nt, 'neg', like),\n",
              "  (like, 'ccomp', embarrassing),\n",
              "  (unreality, 'compound', people),\n",
              "  (people, 'dobj', like),\n",
              "  (bursting, 'acl', people),\n",
              "  (song, 'dobj', bursting),\n",
              "  (expressing, 'acl', song),\n",
              "  (emotion, 'compound', dance),\n",
              "  (dance, 'dobj', expressing),\n",
              "  (prefer, 'ccomp', see),\n",
              "  (solid, 'amod', people),\n",
              "  (commonplace, 'amod', people),\n",
              "  (people, 'dobj', prefer),\n",
              "  (floating, 'amod', head),\n",
              "  (space, 'compound', kick),\n",
              "  (kick, 'compound', head),\n",
              "  (head, 'appos', people),\n",
              "  (fight, 'acl', people),\n",
              "  (choreography, 'compound', dance),\n",
              "  (dance, 'compound', fight),\n",
              "  (fight, 'ccomp', prefer),\n",
              "  (leung, 'compound', fala),\n",
              "  (fala, 'compound', chen),\n",
              "  (chen, 'nsubj', meet),\n",
              "  (meet, 'ccomp', see),\n",
              "  (fall, 'compound', challenge),\n",
              "  (love, 'compound', challenge),\n",
              "  (challenge, 'compound', dance),\n",
              "  (dance, 'compound', graceful),\n",
              "  (graceful, 'amod', rogersthere),\n",
              "  (astaire, 'compound', rogersthere),\n",
              "  (rogersthere, 'dobj', meet),\n",
              "  (problem, 'appos', rogersthere),\n",
              "  (aplenty, 'compound', film),\n",
              "  (film, 'appos', rogersthere),\n",
              "  (there, 'advmod', film),\n",
              "  (immense, 'amod', exposition),\n",
              "  (amount, 'compound', exposition),\n",
              "  (exposition, 'appos', film),\n",
              "  (starting, 'advcl', meet),\n",
              "  (twenty, 'nummod', minute),\n",
              "  (minute, 'compound', chinese),\n",
              "  (chinese, 'dobj', starting),\n",
              "  (i, 'nsubj', m),\n",
              "  (m, 'ccomp', see),\n",
              "  (sure, 'acomp', m),\n",
              "  (ben, 'compound', kingsley),\n",
              "  (kingsley, 'nmod', man),\n",
              "  (fake, 'amod', man),\n",
              "  (mandarin, 'compound', man),\n",
              "  (iron, 'compound', man),\n",
              "  (man, 'dobj', sure),\n",
              "  (present, 'amod', man),\n",
              "  (except, 'prep', present),\n",
              "  (certain, 'amod', mcu),\n",
              "  (remnant, 'amod', mcu),\n",
              "  (star, 'compound', power),\n",
              "  (power, 'nmod', mcu),\n",
              "  (connecting, 'amod', mcu),\n",
              "  (rest, 'compound', mcu),\n",
              "  (mcu, 'pobj', except),\n",
              "  (still, 'advmod', expectation),\n",
              "  (expectation, 'acl', mcu),\n",
              "  (set, 'xcomp', expectation),\n",
              "  (particularly, 'advmod', high),\n",
              "  (high, 'amod', time),\n",
              "  (good, 'amod', time),\n",
              "  (time, 'npadvmod', set)]]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "print(\"Dependency Parsing\")\n",
        "dependencies = parse_dependencies(docs)\n",
        "dependencies[:2] # show only a subset to prevent clutter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMTukthnOzlW"
      },
      "source": [
        "### Constituency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I3j-ihfOzlW",
        "outputId": "5bce6557-b5ba-49a6-c4e9-5f3d177c6049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constituency Parsing\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(S (NP (NP (JJ big) (NN fan)) (NP (JJ many) (JJ many) (NN marvel) (NN film) (NN reason))) (VP (VBD saw) (S (NP (CD one) (JJS oldest) (NN daughter)) (VP (VBD insisted) (S (VP (VB watch)))))) (CC itand) (RB overall) (VBN impressed) (NP (NN movie)) (IN though) (S (VP (VB think) (NP (ADJP (JJ enjoyable)) (NML (JJ insane) (NML (NN eye) (NN candy))) (NN story)))) (NP (PRP itselfi)) (VP (MD could) (VP (VB talk) (NP (NN plot)) (VP (VBN noticed) (SBAR (S (NP (NN review) (NN film)) (ADVP (RB ill)) (VB say) (SBAR (S (VBD looked) (ADJP (ADJP (JJ great)) (MD would) (ADJP (JJ great))) (VBN seen) (NP (NML (JJ big) (NN screen)) (NN story)) (SBAR (IN though) (S (ADJP (JJ enjoyable)) (VP (VBD was) (RB nt) (NP (NP (PRP one)) (VP (ADJP (ADJP (RB particularly) (VBN grabbed)) (VBN impacted) (ADJP (RB mepossibly) (JJ violent))) (JJ frenetic) (NN film) (NN consequence) (VP (VBD lacked) (NP (NN intimacy) (NN humanity))))))))))))))))',\n",
              " '(SINV (JJ okay) (NN movie) (FW simu) (FW liu) (NN son) (ADJP (ADJP (RB seemingly) (JJ immortal)) (NN master)) (CD ten) (NN ring) (RB always) (JJ good) (VB see) (NP (RB tony) (NN leung) (NN hope) (VBN soaked) (NN disney) (NN wind)) (VBG opposing) (NN dad) (FW awkwafina) (FW slacker) (RB sortof) (NN girlfriend) (JJ fine) (VB lead) (NN menger) (FW zhang) (NN sister) (NNP michelle) (FW yeoh) (NN aunt) (ADJP (RB always) (JJ good)) (S (VP (VB see) (NP (NP (NN hope) (VBN soaked) (FW disney)) (PRP tooi)) (VP (VBD grown) (ADJP (RB increasingly) (JJ fond)) (NML (JJ martial) (NN art)) (NN film) (NP (NP (JJ last) (NN couple) (NN decade)) (NN decline) (NP (NP (NP (NN movie) (JJ musical) (NN reason)) (RB do) (RB nt) (VB make) (NP (NP (NN movie)) (PP (PP (IN like) (NP (VB try))) (NP (NP (PRP they)) (VBP re) (ADVP (RB often)) (NP (JJ embarrassing) (NNS people) (NP (RB do) (RB nt) (IN like) (NP (NP (JJ unreality) (NNS people) (VBG bursting) (NN song)) (PP (VP (VBG expressing) (NP (NN emotion))) (NN dance) (NP (. prefer) (NP (JJ solid) (NP (. commonplace) (NNS people) (NP (VBG floating) (NN space) (VB kick) (NP (NN head) (NN fight) (NP (NN choreography) (NN dance) (NP (VB fight) (NP (NN leung) (NN fala) (NN chen) (VB meet) (NP (NP (NN fall) (NN love) (NN challenge)) (NP (NN dance) (NP (ADVP (JJ graceful) (NN astaire) (ADVP (RB rogersthere) (NN problem) (ADVP (JJ aplenty) (NN film) (ADVP (RB there))))) (NP (JJ immense) (NN amount) (NP (NN exposition)))))))))))))))))))))) (VP (VBG starting) (NP (NP (CD twenty) (NN minute) (NN chinese)) (PP (VBP i) (NP (VBP m) (ADJP (RB sure) (NP (NP (VBN ben) (NNP kingsley)) (NP (NP (NP (JJ fake) (FW mandarin) (NN iron) (NN man)) (ADJP (JJ present))) (PP (IN except) (NP (NP (JJ certain) (NN remnant) (NN star) (NN power)) (VP (VBG connecting) (NP (NP (NN rest)) (NP (VBP mcu) (ADVP (RB still)) (NP (NP (NN expectation)) (VP (VBN set) (NP (ADVP (RB particularly) (JJ high)) (JJ good) (NN time))))))))))))))))))))))']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "print(\"Constituency Parsing\")\n",
        "constituents = parse_constituents(docs)\n",
        "constituents[:2] # show only a subset to prevent clutter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiR_4-r2OzlX"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0t0O1IxkOzlX",
        "outputId": "11d67fb0-1bc0-4c3c-a1b6-77f6027957e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('one', 'CARDINAL'), ('one', 'CARDINAL')],\n",
              " [('ten', 'CARDINAL'),\n",
              "  ('tony leung', 'PERSON'),\n",
              "  ('michelle', 'PERSON'),\n",
              "  ('last couple decade', 'DATE'),\n",
              "  ('twenty minute', 'QUANTITY'),\n",
              "  ('chinese', 'NORP'),\n",
              "  ('ben kingsley', 'PERSON'),\n",
              "  ('mandarin iron', 'LOC')]]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "def recognize_entities(docs: List[str]):\n",
        "    def extract_entities(doc):\n",
        "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "    return [extract_entities(doc) for doc in docs]\n",
        "\n",
        "\n",
        "entities = recognize_entities(docs)\n",
        "entities[:2] # show only a subset to prevent clutter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWOtvT2rHNWy"
      },
      "source": [
        "A constituency parsing tree shows the entire structure of text and dependencies within the text, while a dependency tree shows the relationships between words in the text. \n",
        "\n",
        "As an example the first sentence constituency tree is: \n",
        "```\n",
        "'(S (NP (NP (JJ big) (NN fan)) (NP (JJ many) (JJ many) (NN marvel) (NN film) (NN reason))) (VP (VBD saw) (S (NP (CD one) (JJS oldest) (NN daughter)) (VP (VBD insisted) (S (VP (VB watch)))))) (CC itand) (RB overall) (VBN impressed) (NP (NN movie)) (IN though), ...)\n",
        "```\n",
        ", while its dependency tree is:\n",
        "```\n",
        "[(big, 'amod', fan),\n",
        "  (fan, 'nsubj', saw),\n",
        "  (many, 'amod', reason),\n",
        "  (many, 'amod', reason),\n",
        "  (marvel, 'amod', reason),\n",
        "  (film, 'compound', reason),...]\n",
        "```\n",
        "Constituency tree is thus useful for visualizing the entire structure of text as it is information rich. A dependency tree is then useful when coincise information is needed. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Two.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}