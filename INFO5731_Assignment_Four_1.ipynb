{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakhakhini/1/blob/main/INFO5731_Assignment_Four_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kjPgQP7sgnUK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA and LSA. The following information should be reported:\n",
        "\n",
        "(1) Features (top n-gram phrases) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMMex1Wia78G",
        "outputId": "9d2fbab3-0274-4bec-bad0-845f0eef99f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 4.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        " !pip install -q gensim\n",
        " !pip install -q pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ak53-ya78H",
        "outputId": "a6490726-782d-4ac3-987d-d04a3973e227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
            "  from collections import Iterable\n",
            "/usr/local/lib/python3.7/dist-packages/nltk/decorators.py:70: DeprecationWarning: `formatargspec` is deprecated since Python 3.5. Use `signature` and the `Signature` object directly\n",
            "  formatvalue=lambda value: \"\")[1:-1]\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/io/matlab/mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  from .mio5_utils import VarReader5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from pprint import pprint\n",
        "from typing import List, Tuple, Union\n",
        "\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# Prepare matplotlib\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Prepare spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "# Prepare a list of stopwords for preprocessing step\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "\n",
        "def word_tokenize(sentences: List[str]):\n",
        "    return [gensim.utils.simple_preprocess(s, deacc=True) for s in sentences]\n",
        "\n",
        "\n",
        "def to_bigrams(sentences: List[str],\n",
        "               bigram_model: gensim.models.phrases.Phraser):\n",
        "    return [bigram_model[s] for s in sentences]\n",
        "\n",
        "\n",
        "def to_trigrams(sentences: List[str],\n",
        "                bigram_model: gensim.models.phrases.Phraser,\n",
        "                trigram_model: gensim.models.phrases.Phraser):\n",
        "    return [trigram_model[bigram_model[s]] for s in sentences]\n",
        "\n",
        "\n",
        "def lemmatize(sentences: List[str],\n",
        "              pos_tags: List[str] = ['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    lemmatized = []\n",
        "    for sentence in sentences:\n",
        "        doc = nlp(' '.join(sentence))\n",
        "        lemmatized.append(\n",
        "            [token.lemma_ for token in doc if token.pos_ in pos_tags])\n",
        "\n",
        "    return lemmatized\n",
        "\n",
        "\n",
        "def visualize_topics(model: Union[gensim.models.LdaModel,\n",
        "                                  gensim.models.LsiModel],\n",
        "                     id2word: gensim.corpora.Dictionary,\n",
        "                     corpus: List[Tuple[int, int]]):\n",
        "    # Visualize the topics\n",
        "    pyLDAvis.enable_notebook()\n",
        "    topics_vis = pyLDAvis.gensim_models.prepare(model, corpus, id2word)\n",
        "    return topics_vis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EidrP5dsa78I",
        "outputId": "9950b4ea-d1e2-45f9-d6ad-003150035820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0,\n",
            "  '0.022*\"step\" + 0.021*\"aesthetic\" + 0.017*\"budget\" + 0.017*\"sadly\" + '\n",
            "  '0.017*\"brand\" + 0.015*\"sign\" + 0.014*\"rarely\" + 0.013*\"fear\" + '\n",
            "  '0.013*\"reach\" + 0.013*\"exaggerated\"'),\n",
            " (1,\n",
            "  '0.032*\"must\" + 0.027*\"combine\" + 0.024*\"engage\" + 0.020*\"legend_ten\" + '\n",
            "  '0.018*\"ring\" + 0.016*\"familiar\" + 0.016*\"strange\" + 0.016*\"lack\" + '\n",
            "  '0.014*\"simple\" + 0.013*\"similar\"'),\n",
            " (2,\n",
            "  '0.022*\"power\" + 0.019*\"creature\" + 0.016*\"father\" + 0.015*\"satisfy\" + '\n",
            "  '0.013*\"evil\" + 0.012*\"effort\" + 0.011*\"sister\" + 0.011*\"friend\" + '\n",
            "  '0.011*\"totally\" + 0.010*\"praise\"'),\n",
            " (3,\n",
            "  '0.049*\"emotion\" + 0.036*\"display\" + 0.016*\"unnecessary\" + 0.016*\"handle\" + '\n",
            "  '0.015*\"double\" + 0.015*\"personally\" + 0.014*\"visible\" + 0.014*\"account\" + '\n",
            "  '0.011*\"song\" + 0.010*\"distinct\"'),\n",
            " (4,\n",
            "  '0.023*\"slow\" + 0.022*\"ring\" + 0.021*\"genuinely\" + 0.019*\"mystical\" + '\n",
            "  '0.019*\"beautifully\" + 0.017*\"slattery\" + 0.016*\"major\" + 0.016*\"human\" + '\n",
            "  '0.014*\"furthermore\" + 0.014*\"ground\"'),\n",
            " (5,\n",
            "  '0.070*\"movie\" + 0.026*\"good\" + 0.021*\"character\" + 0.020*\"film\" + '\n",
            "  '0.018*\"see\" + 0.017*\"marvel\" + 0.016*\"story\" + 0.016*\"scene\" + '\n",
            "  '0.016*\"great\" + 0.015*\"watch\"'),\n",
            " (6,\n",
            "  '0.026*\"even\" + 0.020*\"bad\" + 0.017*\"learn\" + 0.016*\"guy\" + 0.015*\"lose\" + '\n",
            "  '0.013*\"comic\" + 0.012*\"home\" + 0.011*\"dance\" + 0.010*\"reference\" + '\n",
            "  '0.010*\"woman\"'),\n",
            " (7,\n",
            "  '0.026*\"possible\" + 0.022*\"attention\" + 0.015*\"hurt\" + 0.014*\"accept\" + '\n",
            "  '0.012*\"local\" + 0.012*\"astonish\" + 0.011*\"one\" + 0.011*\"hire\" + '\n",
            "  '0.011*\"advertisement\" + 0.010*\"picture\"'),\n",
            " (8,\n",
            "  '0.021*\"art\" + 0.015*\"common\" + 0.014*\"industry\" + 0.013*\"stellar\" + '\n",
            "  '0.013*\"significant\" + 0.013*\"journey\" + 0.013*\"understandably\" + '\n",
            "  '0.012*\"ago\" + 0.011*\"myth\" + 0.011*\"sequel\"'),\n",
            " (9,\n",
            "  '0.027*\"iron_man\" + 0.025*\"mystery\" + 0.024*\"natural\" + 0.018*\"attractive\" + '\n",
            "  '0.017*\"tree\" + 0.016*\"profound\" + 0.016*\"automatically\" + '\n",
            "  '0.015*\"previously\" + 0.011*\"enter\" + 0.011*\"choose\"')]\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('./cleaned_reviews_github.csv')\n",
        "\n",
        "# Remove stopwords from the text\n",
        "df['review'] = df['review'].apply(\n",
        "    lambda x: ' '.join([w for w in x.split() if w not in stop_words]))\n",
        "\n",
        "# Tokenize the sentences\n",
        "data_words = word_tokenize(df['review'])\n",
        "\n",
        "# Build bigram & trigram models\n",
        "bigram_phrases = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
        "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words],\n",
        "                                        threshold=100)\n",
        "\n",
        "bigram_model = gensim.models.phrases.Phraser(bigram_phrases)\n",
        "trigram_model = gensim.models.phrases.Phraser(trigram_phrases)\n",
        "\n",
        "# Transform sentences into bigrams\n",
        "data_bigrams = to_bigrams(data_words, bigram_model)\n",
        "\n",
        "# Lemmatize the tokens and keep only nouns, adj, verb, and adv\n",
        "data_lemmatized = lemmatize(data_bigrams)\n",
        "\n",
        "# Create corpus\n",
        "id2word = gensim.corpora.Dictionary(data_lemmatized)\n",
        "corpus = [id2word.doc2bow(x) for x in data_lemmatized]\n",
        "\n",
        "# Create model\n",
        "model = gensim.models.LdaModel(corpus=corpus,\n",
        "                               num_topics=10,\n",
        "                               id2word=id2word,\n",
        "                               random_state=100,\n",
        "                               update_every=1,\n",
        "                               chunksize=100,\n",
        "                               passes=10,\n",
        "                               alpha='auto',\n",
        "                               per_word_topics=True)\n",
        "\n",
        "pprint(model.print_topics())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "wLOFamjva78J",
        "outputId": "8920bc34-cff6-4f04-fccd-b55bfc20c8f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyLDAvis/_prepare.py:247: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  by='saliency', ascending=False).head(R).drop('saliency', 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
              "topic                                                \n",
              "5     -0.434029  0.122400       1        1  72.991057\n",
              "6     -0.110927 -0.182786       2        1   6.107875\n",
              "2     -0.042044 -0.233455       3        1   5.291830\n",
              "1      0.073748  0.198598       4        1   3.906335\n",
              "4      0.078606 -0.009377       5        1   3.332214\n",
              "7      0.094411  0.032068       6        1   2.065921\n",
              "8      0.079360  0.027930       7        1   2.030911\n",
              "0      0.090763  0.015137       8        1   1.682295\n",
              "3      0.085673  0.013758       9        1   1.497013\n",
              "9      0.084439  0.015727      10        1   1.094549, topic_info=              Term         Freq        Total Category  logprob  loglift\n",
              "14           movie  4363.000000  4363.000000  Default  30.0000  30.0000\n",
              "161           even   415.000000   415.000000  Default  29.0000  29.0000\n",
              "223            bad   458.000000   458.000000  Default  28.0000  28.0000\n",
              "115       sequence   234.000000   234.000000  Default  27.0000  27.0000\n",
              "48            good  1617.000000  1617.000000  Default  26.0000  26.0000\n",
              "...            ...          ...          ...      ...      ...      ...\n",
              "1344        rushed     3.551652     4.562818  Topic10  -5.5684   4.2643\n",
              "5082  anticipation     3.404644     4.416659  Topic10  -5.6107   4.2546\n",
              "5086          hall     3.403407     4.416976  Topic10  -5.6110   4.2541\n",
              "2684   environment     5.110014     9.159731  Topic10  -5.2046   3.9312\n",
              "663          skill     4.127948    29.166322  Topic10  -5.4180   2.5596\n",
              "\n",
              "[382 rows x 6 columns], token_table=      Topic      Freq         Term\n",
              "term                              \n",
              "300       5  0.956022  abomination\n",
              "2110      9  0.945056     absolute\n",
              "1420      9  0.856037       accent\n",
              "2717      6  0.976414       accept\n",
              "3330      9  0.930663      account\n",
              "...     ...       ...          ...\n",
              "2535      6  0.939277         wrap\n",
              "119       1  0.797208         year\n",
              "119       2  0.192234         year\n",
              "119       3  0.005654         year\n",
              "427       3  0.953233        young\n",
              "\n",
              "[404 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[6, 7, 3, 2, 5, 8, 9, 1, 4, 10])"
            ],
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el611400747160191529066011950\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el611400747160191529066011950_data = {\"mdsDat\": {\"x\": [-0.43402869771148295, -0.11092748304845787, -0.04204356404760279, 0.07374784402571224, 0.07860571917218263, 0.09441061754361604, 0.07936033941138793, 0.09076346976598126, 0.0856732267045507, 0.08443852818411254], \"y\": [0.12239978295927163, -0.18278608293899948, -0.23345486047619335, 0.1985978347198004, -0.009377319348736148, 0.032068367754453514, 0.027930225126608234, 0.015136697581847278, 0.013758018739685591, 0.015727335882261825], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [72.9910567879147, 6.1078753158456225, 5.291829564308698, 3.9063354836920166, 3.3322143115784457, 2.065921333472023, 2.0309107563281787, 1.6822945034072094, 1.4970127172963248, 1.0945492261567773]}, \"tinfo\": {\"Term\": [\"movie\", \"even\", \"bad\", \"sequence\", \"good\", \"ring\", \"get\", \"must\", \"creature\", \"character\", \"world\", \"film\", \"power\", \"combine\", \"find\", \"emotion\", \"people\", \"see\", \"engage\", \"life\", \"marvel\", \"learn\", \"year\", \"legend_ten\", \"guy\", \"story\", \"scene\", \"father\", \"know\", \"great\", \"movie\", \"good\", \"character\", \"film\", \"see\", \"marvel\", \"story\", \"scene\", \"great\", \"watch\", \"really\", \"make\", \"action\", \"fight\", \"feel\", \"time\", \"much\", \"well\", \"also\", \"new\", \"act\", \"amazing\", \"think\", \"say\", \"end\", \"plot\", \"first\", \"lot\", \"could\", \"thing\", \"go\", \"would\", \"way\", \"get\", \"bad\", \"know\", \"learn\", \"lose\", \"comic\", \"home\", \"dance\", \"woman\", \"rate\", \"otherworldly\", \"day\", \"kill\", \"unfortunately\", \"read\", \"be\", \"relate\", \"doubt\", \"fake\", \"sorry\", \"barely\", \"basically\", \"awful\", \"subtitle\", \"master\", \"soul\", \"seek\", \"lame\", \"single\", \"exist\", \"guess\", \"otherwise\", \"racist\", \"guy\", \"bridge\", \"thank\", \"monster\", \"multiple\", \"reference\", \"even\", \"bad\", \"train\", \"try\", \"get\", \"people\", \"year\", \"evil\", \"effort\", \"totally\", \"praise\", \"meet\", \"mother\", \"person\", \"positive\", \"heavy\", \"stereotype\", \"clearly\", \"mixture\", \"third_act\", \"masterpiece\", \"hand\", \"struggle\", \"organization\", \"view\", \"young\", \"relatable\", \"son\", \"history\", \"pandemic\", \"portion\", \"smart\", \"logic\", \"child\", \"razor_fist\", \"upset\", \"remarkable\", \"satisfy\", \"power\", \"help\", \"father\", \"creature\", \"exposition\", \"sister\", \"friend\", \"battle\", \"world\", \"life\", \"away\", \"must\", \"combine\", \"engage\", \"strange\", \"lack\", \"simple\", \"similar\", \"chill\", \"poor\", \"date\", \"spectacular\", \"opinion\", \"pure\", \"dull\", \"game\", \"beauty\", \"amazed\", \"truth\", \"beginning\", \"difficult\", \"extremely\", \"sad\", \"personality\", \"numerous\", \"fast\", \"strike\", \"entertaining\", \"blend\", \"night\", \"initially\", \"familiar\", \"legend_ten\", \"case\", \"ring\", \"slow\", \"genuinely\", \"beautifully\", \"mystical\", \"slattery\", \"major\", \"human\", \"ground\", \"furthermore\", \"leaf\", \"movement\", \"deliver\", \"area\", \"compel\", \"abomination\", \"mess\", \"glue\", \"tad\", \"motivation\", \"provide\", \"inevitably\", \"fluidity\", \"poorly\", \"blow\", \"believable\", \"entirely\", \"form\", \"body\", \"list\", \"explain\", \"appearance\", \"ring\", \"battle\", \"possible\", \"attention\", \"hurt\", \"accept\", \"local\", \"astonish\", \"one\", \"hire\", \"picture\", \"apart\", \"advertisement\", \"word\", \"question\", \"annoy\", \"trash\", \"landscape\", \"everywhere\", \"pointless\", \"nowhere\", \"message\", \"sake\", \"territory\", \"innovative\", \"punch\", \"rise\", \"immersion\", \"wrap\", \"bamboo\", \"mythical_creature\", \"trope\", \"often\", \"literally\", \"art\", \"common\", \"industry\", \"stellar\", \"significant\", \"journey\", \"understandably\", \"ago\", \"myth\", \"sequel\", \"mystic\", \"cake\", \"signature\", \"unfamiliar\", \"costume\", \"exceptional\", \"proportion\", \"fully\", \"mid_credit\", \"synergy\", \"extraordinary\", \"minor\", \"charismatic\", \"today\", \"ancient\", \"charisma\", \"large\", \"unexpected\", \"catch\", \"crew\", \"camera\", \"reference\", \"sequence\", \"set\", \"aesthetic\", \"budget\", \"sadly\", \"brand\", \"sign\", \"rarely\", \"fear\", \"exaggerated\", \"recall\", \"comprehend\", \"age\", \"attempt\", \"step\", \"spectacle\", \"legendary\", \"debut\", \"mood\", \"stick\", \"random\", \"fare\", \"physical\", \"majority\", \"covid\", \"will\", \"ton\", \"asleep\", \"inspiration\", \"purely\", \"desire\", \"directly\", \"reach\", \"literally\", \"appearance\", \"emotion\", \"display\", \"unnecessary\", \"handle\", \"double\", \"personally\", \"visible\", \"account\", \"song\", \"distinct\", \"pop\", \"decade\", \"absolute\", \"mythological\", \"compelling\", \"watchable\", \"approach\", \"disappointment\", \"attraction\", \"premise\", \"extent\", \"tribute\", \"pro\", \"failure\", \"accent\", \"comedic_relief\", \"constantly\", \"substance\", \"consistently\", \"unfold\", \"number\", \"obviously\", \"female\", \"iron_man\", \"mystery\", \"natural\", \"attractive\", \"tree\", \"profound\", \"automatically\", \"previously\", \"enter\", \"choose\", \"affect\", \"ultimate\", \"uninteresting\", \"clothe\", \"redemption\", \"responsible\", \"remove\", \"ghost\", \"club\", \"suggest\", \"brother\", \"marry\", \"beforehand\", \"constructive\", \"satisfied\", \"saw\", \"appreciation\", \"rushed\", \"anticipation\", \"hall\", \"environment\", \"skill\"], \"Freq\": [4363.0, 415.0, 458.0, 234.0, 1617.0, 183.0, 472.0, 106.0, 126.0, 1324.0, 191.0, 1271.0, 133.0, 89.0, 185.0, 63.0, 192.0, 1097.0, 79.0, 142.0, 1059.0, 90.0, 176.0, 85.0, 87.0, 1004.0, 991.0, 98.0, 358.0, 981.0, 4362.91461174163, 1616.7194357409562, 1323.3980883723882, 1271.013282096216, 1096.4171429174305, 1058.2201556464495, 1003.0272845256604, 990.7177121618957, 980.9508065081726, 954.0068272938629, 946.2646805357364, 880.1678877547994, 862.7514819478548, 855.657341262215, 724.4800363620079, 572.0723833855551, 502.17898706362115, 490.8169762016779, 461.29961183034794, 439.319190367665, 414.3396601723341, 389.5513183544369, 357.6490985985137, 357.48712597827256, 333.1846153274574, 331.0987663124691, 320.10786203199547, 314.36758704810416, 306.750365827409, 286.4240879428217, 529.7959979149366, 375.3510443627471, 331.83551777599257, 434.05237825799884, 352.46990486254447, 334.7708777842423, 89.2889545700715, 78.96547057047937, 65.84053389676282, 64.49934527848713, 56.875054456977665, 52.56867899703622, 52.07794561903937, 49.26736677819953, 49.22302241009478, 47.31063344942841, 46.854489549093806, 44.22727072875299, 44.07045735854593, 41.76063762098173, 37.630993982554955, 34.52895333084883, 34.367499495902265, 34.086820296955445, 33.13113121377211, 32.068895222361945, 30.743301370686073, 36.85806521416173, 27.830803995518007, 27.12107647070565, 27.041995116679594, 24.733082192183304, 24.736924661423046, 24.682893111584864, 24.636563427403438, 23.925946016700436, 83.00139810104817, 28.391330543557366, 33.11501170467699, 40.79956085909094, 32.00087698776436, 52.90601829463407, 134.3072468015486, 105.37384516950992, 32.95297509119878, 39.67940732157559, 37.64867321012688, 35.37948353434128, 33.89471376214543, 56.30746554954671, 53.39054630893778, 47.99347912125628, 46.18593809915717, 46.080726989767946, 44.96600629786327, 42.49491750959828, 38.85733422451352, 37.642933454499, 37.30000951517787, 37.02224699057305, 34.26835070674843, 33.328890378333675, 33.105929060234374, 32.92001936644405, 32.673077675608496, 31.160778494009588, 30.68257474798046, 29.449114966015763, 28.014468111439083, 28.006797995714162, 26.673461197404663, 26.445547092205196, 26.381850143725355, 26.13838001305265, 25.94335040291418, 25.711667235111836, 25.361737322836838, 24.534039442754718, 24.17491419097776, 67.04904674740524, 100.94408636419769, 32.05663795425956, 72.80680840748046, 87.63929118587092, 41.74744891429356, 51.64994333584472, 49.61785920450391, 40.43424381278842, 37.562084023790284, 35.28034506316438, 31.708738181874597, 105.2846387205715, 88.93204604326262, 78.59847926054525, 53.28910797738596, 51.587134579181594, 48.07768167711693, 43.19890737446948, 41.75037752024738, 41.51228200605886, 34.84651698418509, 34.66679161716259, 34.59432615121984, 34.198862295270644, 34.15506071326555, 33.43857496691454, 31.32667122034957, 30.90818222659613, 29.504901531554346, 27.408157134852168, 27.124960930937196, 26.254450498651188, 23.759967260930626, 23.469904344774196, 23.413787530882004, 23.033943898725312, 22.944028632282482, 22.049835775547496, 21.632348950607675, 21.417815229436858, 20.82969679152403, 53.45038910787881, 67.35557419024141, 31.954474306036232, 59.40786822761423, 66.15247411978655, 60.60900024148219, 52.69366136650824, 54.44875808702872, 48.535918755097015, 44.49335251901075, 44.04625382994289, 39.68112672471313, 39.90600456061002, 37.90859207089011, 31.945127587456522, 31.555477244855044, 28.47298678283398, 25.316406872010457, 25.1750168686993, 24.817395437031333, 24.729593635471524, 24.387874522462116, 20.658176577521637, 20.303758300798336, 20.090920000331842, 19.94149968229738, 18.378468248339487, 18.02864890000873, 17.00400870955478, 16.019951489614733, 15.76556848059699, 14.785494096232705, 14.621576467783532, 14.004929563238564, 24.310542341017985, 62.28162231846039, 24.737092289798323, 46.37621740539972, 39.4017366012957, 25.54416758904035, 24.61927006852155, 21.17286180213646, 20.880987526010895, 20.163816809212456, 19.922571698540644, 18.25698829956274, 18.165068963988848, 18.491964881574813, 15.411295490245784, 13.900007540183196, 13.589267152097108, 13.352888657490723, 11.147832279514393, 10.386989613439091, 10.239076383878146, 9.96850970693248, 9.939202863194621, 9.930833919399422, 9.530236986616249, 9.20794840487844, 8.923079602131088, 8.673709287426682, 8.657058099214929, 8.597656756063412, 8.601874762626336, 8.500182934825602, 8.055683399701627, 11.092647307088468, 9.637877536878133, 36.19639433360329, 26.416708883349408, 24.23749238592384, 23.106353756405085, 22.87451821094521, 22.34774789321225, 22.06986557522142, 20.267374200620633, 19.674767643891464, 19.275884300814397, 18.306985204267047, 18.117550834029768, 17.84065750692762, 17.315836224897232, 17.17984274118467, 16.64627974456749, 15.755755361746523, 15.56451025371956, 15.656071512628378, 15.650566934072344, 14.257180083180275, 13.230538391771253, 13.046274916172282, 12.679196876829142, 11.162311597842503, 10.568615543020517, 10.456948822996651, 9.87896003403679, 9.579748479213373, 8.579123680637768, 11.655731213070768, 16.198993740094558, 14.45916900930039, 10.614440395644746, 30.45891168372229, 24.772420862650712, 24.161734188346177, 23.94559718474717, 20.74797079584098, 20.69794741984161, 19.29960404029909, 18.57862030434506, 16.36676466266769, 15.893613299751294, 12.800659955945473, 11.69597096395347, 31.609213554604672, 10.81981459043444, 10.782986739763269, 9.593702388468312, 9.104109901852818, 8.801754406746902, 8.47909344118466, 8.39944873500933, 8.18074016249794, 7.962234731275926, 7.3515447267863205, 7.324072032018633, 7.30880113535374, 7.220493951769843, 7.179518022573332, 6.7129877336165364, 6.548270119598966, 6.526277577385306, 19.108780439467267, 13.585950897708509, 10.474126061138248, 62.2071808835177, 46.31753432963397, 20.078339753303105, 19.781631228031785, 19.718868561753705, 19.26195450208106, 17.649772591285075, 17.268554707215888, 13.833261244261587, 12.672012045135693, 12.282634113573526, 11.217239394476659, 10.643302620008917, 10.09117773309468, 9.896890013112024, 9.541117750702925, 8.532705269121859, 8.100733051303303, 8.095354461021577, 7.7963682583450025, 6.581132237161581, 6.565234724530539, 6.0735171851307905, 6.053866216603972, 6.012820016057319, 5.90007468815624, 5.787295577374052, 5.497438457464354, 5.221758293977673, 5.148127615578823, 7.116699956239976, 6.702300170736886, 5.851372219922609, 24.70388625887206, 22.89859033610275, 21.951179791439078, 16.562223414720602, 15.975170385864226, 15.194227631255032, 14.832408883349524, 13.689811184433022, 10.422862820885559, 10.195307555617365, 7.682018680693515, 6.49083588119719, 6.3288345559221515, 6.251323386603389, 6.07592136696148, 4.712168443363522, 4.652146558223796, 4.514826275105825, 4.336854099006475, 4.231902360246717, 4.094191201875901, 4.055659222128346, 3.8434902281766092, 3.7594966831291563, 3.6655570529339734, 3.5998477595282408, 3.597291903384928, 3.5516515841584253, 3.4046439215755915, 3.4034065110606324, 5.110013582644147, 4.127947743390275], \"Total\": [4363.0, 415.0, 458.0, 234.0, 1617.0, 183.0, 472.0, 106.0, 126.0, 1324.0, 191.0, 1271.0, 133.0, 89.0, 185.0, 63.0, 192.0, 1097.0, 79.0, 142.0, 1059.0, 90.0, 176.0, 85.0, 87.0, 1004.0, 991.0, 98.0, 358.0, 981.0, 4363.888717309462, 1617.6935111647972, 1324.3723734674652, 1271.9873875064618, 1097.3912544866532, 1059.194250928794, 1004.0013714389708, 991.6917936541425, 981.9248905785273, 954.9809294742009, 947.2387990156179, 881.1420228606637, 863.7255674171912, 856.6471692442747, 725.4541929644944, 573.0483523125288, 503.15314813596626, 491.7910571288091, 462.27373673154517, 440.2933322411783, 415.3137751927166, 390.5254055561019, 358.6232024129703, 358.4612181123434, 334.15875257787, 332.07289699802453, 321.08206706505325, 315.34169185016435, 307.724505559208, 287.39823414551506, 541.8688426356177, 380.3468386697204, 336.9090796259634, 472.5466218366722, 458.6892835344133, 358.6147439972902, 90.25733613634718, 79.93371915058506, 66.80866795205853, 65.46749162474403, 57.843724250767295, 53.53694246104385, 53.04607994391124, 50.23702571624799, 50.20942754867127, 48.27879366279354, 47.822707566966095, 45.195390150485466, 45.043256015164864, 42.72908157463415, 38.59925513412265, 35.497335224193954, 35.33563675198392, 35.055800030198114, 34.0992612062508, 33.03699529556914, 31.711951144782063, 38.09053033446063, 28.79893701207057, 28.089782065221346, 28.010098487775515, 25.701240155266976, 25.70663674675933, 25.650958927133217, 25.604663394980197, 24.89431075724315, 87.98408877726473, 29.713410338685325, 35.513466539950635, 46.3309012535935, 35.65935839054821, 69.97760272619631, 415.61472497110555, 458.6892835344133, 42.871440958990085, 103.6858024917517, 472.5466218366722, 192.22916693181753, 176.867350512206, 57.281104156745464, 54.36499103826449, 48.967134106751196, 47.1596926765111, 47.05441164591819, 45.93968162451451, 43.46856415912423, 39.83097691017717, 38.62085312323854, 38.27376153421888, 37.99600530234014, 35.24244328165788, 34.302667301600216, 34.079708698383484, 33.8937388549732, 33.64690851062174, 32.13442161155212, 31.65653076154949, 30.422778751777752, 28.988204123778647, 28.98043935826496, 27.64708288284947, 27.419405648106114, 27.356273831335066, 27.11314218663414, 26.917049644419404, 26.685382605128993, 26.335455922653107, 25.50984843686665, 25.14989711959179, 75.4351349998392, 133.01525301782692, 35.04200247201922, 98.47580359561375, 126.87189905144788, 51.579373406201725, 77.15482126443196, 80.45880038348491, 85.28975451049848, 191.92375014547707, 142.9614850726608, 94.09010858353344, 106.27211093018707, 89.91983670290041, 79.5980695179192, 54.27671235642758, 52.57466450746489, 49.065178285878616, 44.18648593897779, 42.73907458236953, 42.49981957743602, 35.83409872371682, 35.654279000083775, 35.58176685656343, 35.18641339254243, 35.14261989689137, 34.42604135228094, 32.31437499717017, 31.895858963191362, 30.492628995242576, 28.396044280156495, 28.11273509177903, 27.24191728293651, 24.74749336053906, 24.457471002316353, 24.401870479065114, 24.021392621830653, 23.93197818856617, 23.037274813099575, 22.61980200341397, 22.40533150705059, 21.817957458448422, 59.077369953481586, 85.05926663823386, 35.88961198841469, 183.61966442088115, 67.1276687001778, 61.58423210052422, 53.66880152203774, 55.475054334716425, 49.51127750180722, 45.47800648376279, 45.03206330811289, 40.656280959790315, 40.8881349467712, 38.88380515918538, 32.92039565878066, 32.53082861478388, 29.448387392182337, 26.291779772448223, 26.150031648608962, 25.79244282418728, 25.705863123739636, 25.36397371989904, 21.63320261435603, 21.278891826204532, 21.074278747217544, 20.93190981844874, 19.353729189961488, 19.003683750121947, 17.97907624623791, 16.99498754069835, 16.740711327049127, 15.760806190511714, 15.596778034056811, 14.979987582553019, 35.66282617545226, 183.61966442088115, 85.28975451049848, 47.36139845707494, 40.38614833830204, 26.528990075673608, 25.60388167265061, 22.158499961136812, 21.86614597393308, 21.148087873365533, 20.908557825974395, 19.241185289633165, 19.14930541524422, 19.497610424595628, 16.39548318164704, 14.884275175620628, 14.573492552128492, 14.337089807238984, 12.132285329290186, 11.371508399178996, 11.223236381331967, 10.952841692800042, 10.923457101838757, 10.91525929293199, 10.515194277055011, 10.19233101331377, 9.907242111055417, 9.657986371740888, 9.643826776266721, 9.581842420479713, 9.587109329834446, 9.484533350702916, 9.039922886590118, 27.645110974260295, 24.111208517909816, 37.1975150053151, 27.418110454859544, 25.238547246411937, 24.107377838175683, 23.87598912756058, 23.34936060094239, 23.071173236281698, 21.268597261179668, 20.67605441351046, 20.27667991248513, 19.30791361838473, 19.11853726668253, 18.844316355001688, 18.319562905277323, 18.180703794595292, 17.647117064496506, 16.760219763748097, 16.565396915677688, 16.664457551015847, 16.666306430959768, 15.258161713779877, 14.231427781930073, 14.047234030273762, 13.68004828433391, 12.16312312439302, 11.569569127639822, 11.457893332117207, 10.879793987426, 10.580587661766971, 9.579949730334782, 15.931549104911035, 69.97760272619631, 234.14090161369336, 72.56245868694845, 31.45887583457128, 25.772555900371767, 25.161617117614632, 24.946340497687704, 21.74873794817679, 21.698494031411794, 20.30088903383069, 19.58019294450374, 17.37603486026119, 16.936438591470047, 13.800559217246555, 12.69565391794795, 34.44860963054549, 11.81962819793754, 11.782654089195505, 10.593372942507376, 10.103928117990167, 9.801469871306072, 9.478718378636747, 9.399580769743235, 9.18045298793554, 8.962483561187748, 8.351331203492194, 8.323702553043484, 8.3090307691586, 8.220215433044768, 8.179145272145739, 7.712726170291806, 7.548056200653341, 7.526031110244675, 25.14322927011256, 24.111208517909816, 35.66282617545226, 63.20351698214553, 47.314348199377385, 21.074620540636744, 20.77844485826942, 20.72173679315896, 20.25817183900353, 18.647239527849873, 18.26655600012936, 14.829937115520154, 13.668609188639387, 13.27950723029879, 12.213702960049021, 11.6395260625981, 11.087421471238885, 10.893154005222605, 10.537301299163762, 9.528949710810359, 9.096921462617273, 9.09255756849666, 8.792576450823008, 7.577506416521286, 7.561832424357855, 7.069932685745135, 7.051043696458188, 7.00904094451901, 6.896354183937826, 6.783474543207375, 6.493632338916171, 6.221649291388127, 6.144399656553138, 8.635490145577872, 15.773859806417297, 20.078318471968323, 25.715193512367474, 23.910161511924084, 22.96247471324835, 17.57482294432102, 16.98773197100749, 16.209036233502577, 15.849052185272475, 14.701384051706938, 11.433702177760019, 11.206287407541458, 8.693197905801762, 7.501845634029279, 7.34006898226786, 7.262306530175051, 7.087128429787624, 5.723106895224497, 5.663149752216675, 5.525861178834425, 5.347692049056447, 5.2427462158229154, 5.104971427545713, 5.066535094582373, 4.856159898211729, 4.771269906820329, 4.676594733021826, 4.611173338267113, 4.609337881848894, 4.562818405455394, 4.416658895458959, 4.416975536860922, 9.159730768051034, 29.16632159431685], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.6549, -3.6477, -3.8479, -3.8883, -4.036, -4.0715, -4.125, -4.1374, -4.1473, -4.1751, -4.1833, -4.2557, -4.2757, -4.284, -4.4504, -4.6866, -4.8169, -4.8397, -4.9018, -4.9506, -5.0091, -5.0708, -5.1563, -5.1567, -5.2271, -5.2334, -5.2672, -5.2853, -5.3098, -5.3783, -4.7633, -5.108, -5.2312, -4.9627, -5.1709, -5.2224, -4.0632, -4.1861, -4.3678, -4.3884, -4.5142, -4.5929, -4.6023, -4.6578, -4.6587, -4.6983, -4.708, -4.7657, -4.7693, -4.8231, -4.9272, -5.0133, -5.018, -5.0262, -5.0546, -5.0872, -5.1294, -4.948, -5.2289, -5.2548, -5.2577, -5.3469, -5.3468, -5.349, -5.3508, -5.3801, -4.1362, -5.209, -5.0551, -4.8464, -5.0893, -4.5865, -3.6549, -3.8975, -5.06, -4.8742, -4.9268, -4.9889, -5.0318, -4.3808, -4.434, -4.5406, -4.579, -4.5813, -4.6057, -4.6623, -4.7518, -4.7835, -4.7927, -4.8001, -4.8774, -4.9052, -4.9119, -4.9176, -4.9251, -4.9725, -4.988, -5.029, -5.0789, -5.0792, -5.128, -5.1366, -5.139, -5.1482, -5.1557, -5.1647, -5.1784, -5.2116, -5.2263, -4.2062, -3.7971, -4.9441, -4.1238, -3.9384, -4.68, -4.4672, -4.5073, -4.712, -4.7857, -4.8483, -4.9551, -3.4514, -3.6202, -3.7437, -4.1324, -4.1648, -4.2353, -4.3423, -4.3764, -4.3821, -4.5571, -4.5623, -4.5644, -4.5759, -4.5772, -4.5984, -4.6636, -4.6771, -4.7235, -4.7972, -4.8076, -4.8402, -4.9401, -4.9524, -4.9548, -4.9711, -4.975, -5.0148, -5.0339, -5.0439, -5.0717, -4.1293, -3.8981, -4.6438, -4.0237, -3.7572, -3.8447, -3.9846, -3.9519, -4.0668, -4.1538, -4.1639, -4.2682, -4.2626, -4.3139, -4.4851, -4.4974, -4.6002, -4.7177, -4.7233, -4.7376, -4.7411, -4.755, -4.921, -4.9383, -4.9489, -4.9563, -5.0379, -5.0572, -5.1157, -5.1753, -5.1913, -5.2555, -5.2666, -5.3097, -4.7582, -3.8175, -4.7408, -3.6343, -3.7972, -4.2307, -4.2675, -4.4183, -4.4322, -4.4672, -4.4792, -4.5665, -4.5716, -4.5537, -4.736, -4.8392, -4.8618, -4.8793, -5.0598, -5.1305, -5.1448, -5.1716, -5.1746, -5.1754, -5.2166, -5.251, -5.2824, -5.3108, -5.3127, -5.3196, -5.3191, -5.331, -5.3847, -5.0648, -5.2054, -3.865, -4.18, -4.2661, -4.3139, -4.3239, -4.3472, -4.3598, -4.445, -4.4746, -4.4951, -4.5467, -4.5571, -4.5725, -4.6023, -4.6102, -4.6418, -4.6968, -4.709, -4.7031, -4.7035, -4.7967, -4.8714, -4.8855, -4.914, -5.0414, -5.0961, -5.1067, -5.1636, -5.1943, -5.3046, -4.9982, -4.669, -4.7826, -5.0918, -3.8493, -4.0559, -4.0809, -4.0899, -4.2332, -4.2356, -4.3056, -4.3436, -4.4704, -4.4997, -4.7161, -4.8064, -3.8122, -4.8843, -4.8877, -5.0045, -5.0569, -5.0907, -5.128, -5.1375, -5.1639, -5.1909, -5.2707, -5.2745, -5.2766, -5.2887, -5.2944, -5.3616, -5.3864, -5.3898, -4.3155, -4.6566, -4.9167, -3.0185, -3.3134, -4.1493, -4.1642, -4.1674, -4.1908, -4.2782, -4.3001, -4.5219, -4.6096, -4.6408, -4.7315, -4.784, -4.8373, -4.8567, -4.8933, -5.005, -5.057, -5.0577, -5.0953, -5.2647, -5.2672, -5.345, -5.3483, -5.3551, -5.374, -5.3933, -5.4447, -5.4961, -5.5103, -5.1865, -5.2465, -5.3823, -3.6289, -3.7048, -3.747, -4.0287, -4.0648, -4.1149, -4.139, -4.2192, -4.4918, -4.5139, -4.7969, -4.9654, -4.9907, -5.003, -5.0315, -5.2857, -5.2985, -5.3285, -5.3687, -5.3932, -5.4263, -5.4357, -5.4894, -5.5115, -5.5368, -5.5549, -5.5556, -5.5684, -5.6107, -5.611, -5.2046, -5.418], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.3146, 0.3142, 0.3141, 0.3141, 0.3139, 0.3139, 0.3139, 0.3139, 0.3138, 0.3138, 0.3138, 0.3137, 0.3137, 0.3137, 0.3135, 0.3131, 0.3129, 0.3129, 0.3127, 0.3126, 0.3125, 0.3123, 0.3121, 0.3121, 0.3119, 0.3119, 0.3118, 0.3117, 0.3117, 0.3114, 0.2923, 0.3016, 0.2997, 0.2299, 0.0514, 0.246, 2.7848, 2.7834, 2.781, 2.7807, 2.7787, 2.7773, 2.7772, 2.7761, 2.7757, 2.7753, 2.7751, 2.7739, 2.7738, 2.7727, 2.7702, 2.7679, 2.7678, 2.7676, 2.7668, 2.7658, 2.7646, 2.7627, 2.7614, 2.7605, 2.7604, 2.7572, 2.7571, 2.7571, 2.757, 2.7559, 2.7373, 2.7501, 2.7257, 2.6685, 2.6873, 2.5159, 1.666, 1.3247, 2.5325, 1.8351, 0.2658, 1.103, 1.1435, 2.9219, 2.9209, 2.9189, 2.9181, 2.9181, 2.9176, 2.9164, 2.9143, 2.9134, 2.9132, 2.913, 2.911, 2.9102, 2.91, 2.9099, 2.9096, 2.9082, 2.9078, 2.9065, 2.9048, 2.9048, 2.9032, 2.9028, 2.9027, 2.9024, 2.9022, 2.9018, 2.9013, 2.9, 2.8995, 2.8212, 2.6631, 2.85, 2.637, 2.5691, 2.7275, 2.5377, 2.4556, 2.1926, 1.3079, 1.5398, 1.8513, 3.2332, 3.2315, 3.2299, 3.2242, 3.2236, 3.2222, 3.22, 3.2192, 3.2191, 3.2146, 3.2145, 3.2144, 3.2141, 3.2141, 3.2135, 3.2115, 3.2111, 3.2096, 3.2072, 3.2068, 3.2056, 3.2018, 3.2014, 3.2012, 3.2006, 3.2004, 3.1988, 3.1979, 3.1975, 3.1962, 3.1425, 3.0092, 3.1264, 2.1141, 3.3869, 3.3856, 3.3832, 3.3829, 3.3816, 3.3796, 3.3794, 3.3773, 3.3772, 3.3761, 3.3715, 3.3711, 3.3678, 3.3637, 3.3635, 3.363, 3.3628, 3.3623, 3.3554, 3.3546, 3.3537, 3.3531, 3.3498, 3.3489, 3.3458, 3.3424, 3.3415, 3.3377, 3.337, 3.3342, 3.0183, 2.3203, 2.1638, 3.8586, 3.8549, 3.8418, 3.8404, 3.8341, 3.8335, 3.8319, 3.8313, 3.8271, 3.8268, 3.8266, 3.8177, 3.8112, 3.8097, 3.8085, 3.795, 3.789, 3.7878, 3.7854, 3.7852, 3.7851, 3.7812, 3.778, 3.775, 3.7721, 3.7717, 3.7712, 3.7712, 3.77, 3.7643, 2.9664, 2.9626, 3.8694, 3.8595, 3.8562, 3.8543, 3.8538, 3.8528, 3.8523, 3.8485, 3.847, 3.8461, 3.8435, 3.8429, 3.842, 3.8403, 3.8401, 3.8383, 3.8349, 3.8344, 3.8343, 3.8338, 3.8288, 3.8238, 3.8228, 3.8207, 3.8108, 3.8062, 3.8053, 3.8002, 3.7973, 3.7863, 3.5842, 2.4335, 1.1121, 1.9745, 4.0527, 4.0454, 4.0445, 4.0441, 4.0379, 4.0378, 4.0344, 4.0325, 4.0252, 4.0215, 4.0098, 4.003, 3.999, 3.9966, 3.9964, 3.9859, 3.9808, 3.9774, 3.9736, 3.9725, 3.9697, 3.9667, 3.9575, 3.9571, 3.9567, 3.9553, 3.9547, 3.9462, 3.9429, 3.9425, 3.8106, 3.5114, 2.8598, 4.1858, 4.1804, 4.1533, 4.1525, 4.1521, 4.1513, 4.1467, 4.1455, 4.1321, 4.126, 4.1237, 4.1166, 4.1122, 4.1075, 4.1058, 4.1024, 4.0913, 4.0857, 4.0855, 4.0814, 4.0607, 4.0604, 4.0498, 4.0492, 4.0484, 4.0457, 4.0429, 4.0352, 4.0265, 4.0248, 4.0083, 3.3458, 2.9687, 4.4747, 4.4716, 4.4698, 4.4555, 4.4534, 4.4502, 4.4485, 4.4435, 4.4223, 4.4203, 4.3912, 4.3701, 4.3666, 4.3649, 4.3609, 4.3205, 4.3182, 4.3128, 4.3053, 4.3006, 4.2942, 4.2923, 4.281, 4.2765, 4.2712, 4.2672, 4.2669, 4.2643, 4.2546, 4.2541, 3.9312, 2.5596]}, \"token.table\": {\"Topic\": [5, 9, 9, 6, 9, 1, 1, 6, 8, 10, 8, 7, 1, 4, 1, 7, 6, 10, 6, 5, 8, 10, 9, 5, 7, 8, 6, 8, 6, 9, 10, 10, 1, 3, 2, 1, 2, 6, 2, 2, 1, 3, 5, 2, 5, 4, 10, 4, 5, 4, 5, 5, 8, 2, 10, 8, 7, 6, 7, 4, 6, 7, 1, 7, 7, 3, 4, 10, 3, 10, 10, 4, 9, 2, 7, 5, 9, 8, 9, 9, 10, 7, 1, 8, 1, 3, 7, 2, 4, 2, 8, 9, 5, 8, 4, 8, 9, 9, 9, 9, 2, 4, 3, 9, 1, 4, 10, 4, 5, 6, 10, 1, 2, 6, 3, 8, 7, 2, 5, 3, 5, 9, 7, 4, 9, 2, 4, 7, 8, 4, 2, 3, 8, 1, 3, 9, 1, 1, 1, 3, 1, 5, 5, 2, 3, 7, 5, 4, 5, 1, 2, 10, 5, 1, 2, 1, 1, 5, 2, 1, 2, 10, 3, 9, 3, 3, 5, 6, 3, 2, 5, 6, 6, 7, 5, 4, 6, 8, 10, 7, 2, 1, 2, 4, 2, 6, 7, 5, 2, 1, 4, 5, 8, 1, 3, 5, 5, 6, 8, 6, 3, 2, 1, 5, 8, 1, 10, 1, 2, 3, 3, 5, 6, 7, 7, 3, 2, 5, 8, 3, 5, 5, 1, 1, 2, 8, 4, 10, 7, 5, 7, 6, 9, 10, 1, 4, 6, 7, 9, 4, 1, 9, 1, 6, 6, 4, 3, 2, 2, 3, 1, 2, 3, 3, 4, 9, 8, 6, 1, 6, 4, 5, 9, 3, 3, 6, 2, 3, 5, 3, 9, 10, 9, 10, 7, 5, 6, 4, 8, 6, 2, 8, 8, 2, 3, 3, 8, 2, 1, 8, 10, 2, 7, 3, 2, 3, 10, 10, 1, 2, 3, 4, 5, 6, 10, 4, 8, 6, 10, 3, 7, 10, 1, 1, 1, 2, 7, 1, 7, 1, 5, 7, 8, 7, 7, 4, 4, 2, 2, 3, 1, 8, 10, 5, 5, 3, 3, 9, 2, 2, 8, 4, 7, 5, 8, 3, 8, 1, 4, 4, 3, 9, 2, 10, 7, 5, 6, 2, 7, 1, 1, 3, 1, 7, 8, 3, 2, 3, 6, 10, 9, 6, 4, 1, 2, 3, 10, 7, 7, 7, 9, 2, 10, 9, 3, 3, 9, 1, 9, 1, 2, 1, 8, 2, 6, 1, 3, 1, 2, 6, 1, 2, 3, 3], \"Freq\": [0.9560217875044087, 0.9450556612736044, 0.8560372306987208, 0.9764144483882825, 0.9306625726206741, 0.9968366683910087, 0.9991599560733615, 0.9231900529355926, 0.9536259387575423, 0.9202597348739607, 0.9419908132239962, 0.9403535058941022, 0.9972446266565975, 0.9719130008624253, 0.9986546187555873, 0.9043729877188872, 0.9606482419998401, 0.679246478165766, 0.939981874521188, 0.6729696598336304, 0.28040402493067934, 0.8678035983761561, 0.9444902400722843, 0.9508160710841899, 0.9678065858661798, 0.8515591905122614, 0.960388722595851, 0.9452053496067265, 0.9656776296989078, 0.8798404562999868, 0.9672928173363611, 0.94642883528004, 0.6589428042264001, 0.3400995118587872, 0.9686110892866755, 0.7674040197487874, 0.22891313089097348, 0.9387605471435064, 0.9698823010945802, 0.9677629025566896, 0.22277001627037457, 0.46898950793763067, 0.29311844246101915, 0.9768387965822536, 0.987538355560947, 0.9593253777216711, 0.823696106356175, 0.9508366635020333, 0.9455435733833778, 0.9725991410835327, 0.9471847793659739, 0.9517279648442265, 0.9620649570715424, 0.942335453280011, 0.7835499290782625, 0.9700240867317074, 0.9414946211061983, 0.18830560545271927, 0.7532224218108771, 0.8916229022016099, 0.08358964708140093, 0.9451270874239879, 0.9989637555909807, 0.9507700657339853, 0.9254490935356509, 0.9743161784385561, 0.9827072862575641, 0.8923561957968643, 0.9737865784991141, 0.8261837991924276, 0.7479862271997814, 0.989770480723407, 0.8700249204100468, 0.9878957629773606, 0.9482783302228546, 0.9508675417324958, 0.9180077684760178, 0.944708647782558, 0.8036454267714659, 0.8845024716733246, 0.8383512310385477, 0.9350573108755951, 0.9976456033038662, 0.8381897244205665, 0.2995147095937344, 0.6936130116907534, 0.9394621321969593, 0.9854137287718624, 0.9767233234984428, 0.9759123414123992, 0.9439864011464767, 0.9006277650587181, 0.9836822903876897, 0.9273910810831135, 0.9604188248441033, 0.9301051108427356, 0.8794183870746887, 0.9722209382693202, 0.9510843291067903, 0.96517006270453, 0.9844749560052288, 0.967486206200795, 0.9748920948537682, 0.9809580694301313, 0.9965323291132409, 0.9924863816228036, 0.8746073532902799, 0.9549740660944083, 0.9414540588326042, 0.32752054355832627, 0.5458675725972104, 0.6737008656742521, 0.3224139857155349, 0.8793908115762359, 0.9776347859280118, 0.9703683745023257, 0.9633301540341446, 0.9725115053470225, 0.9345802139586287, 0.8142789884095425, 0.1744883546591877, 0.923786746618631, 0.9175417237422767, 0.954411531683403, 0.8509378552020408, 0.9859895053796888, 0.897128630501544, 0.08463477646240981, 0.8511017880448016, 0.9574798747970003, 0.25386946932325954, 0.741298850423918, 0.9359196027492783, 0.9979954723832363, 0.6474645781791696, 0.2988298053134629, 0.9992445323261319, 0.9992237442633787, 0.8298316653703425, 0.1670440365355884, 0.9966299361563721, 0.9554789875108585, 0.9557538916609649, 0.372861636726041, 0.6214360612100683, 0.9658687975569973, 0.978278907856096, 0.9585766676543408, 0.9905132843165021, 0.9184278967293196, 0.08041534579657636, 0.9048363392029067, 0.9725407732725472, 0.9780964659678745, 0.020300115331408713, 0.9995712963178681, 0.9990580841901437, 0.9838578211214305, 0.9746224330644948, 0.04546276554759988, 0.9433523851126975, 0.6791977847656487, 0.9736311517948082, 0.9625359422430686, 0.9839244068157322, 0.9131898220015184, 0.0570743638750949, 0.9565461265412717, 0.9765948948179671, 0.9775844226145763, 0.9770815895986948, 0.9800599240994599, 0.9332394918321048, 0.9509263653601133, 0.949024174914675, 0.9625098976379345, 0.883016847494819, 0.8558351474497776, 0.9721879008212204, 0.942209954953202, 0.9735123111872811, 0.9341501028818028, 0.0641356787053178, 0.9890695544546311, 0.9639380601172697, 0.9066717194198703, 0.8727607868341173, 0.977270610333346, 0.9860694300300666, 0.09405206882425701, 0.7876860764031525, 0.10580857742728914, 0.9335757391101563, 0.734463551121012, 0.244821183707004, 0.013989781926114515, 0.9617370951388999, 0.41474486824548823, 0.5806428155436835, 0.947717581823288, 0.9659305289200025, 0.9883188326465073, 0.9957452760455098, 0.9675006316670787, 0.8926097264651278, 0.9987039287299497, 0.7894941859333383, 0.9988724911150653, 0.9713700406666687, 0.9683181359342223, 0.97759165168502, 0.96927616241746, 0.9154610950334294, 0.9601272619296665, 0.91347124119945, 0.9647458244671556, 0.884938537577444, 0.10791933385090781, 0.89074267897605, 0.979545317005134, 0.9707300566798265, 0.9720417801681198, 0.9997963473940258, 0.9977081567704816, 0.8973801393039602, 0.08412938805974628, 0.9880296823027939, 0.9619341127633043, 0.9322602304819018, 0.9734104931954374, 0.9673025423521471, 0.9489133167877989, 0.901922960711858, 0.9580848873970432, 0.9970625668242692, 0.9372769152463397, 0.913005070325594, 0.11580118593639849, 0.8106083015547894, 0.9425506958465414, 0.5071681946067095, 0.4437721702808708, 0.5787641805777962, 0.3979003741472349, 0.9457119773551033, 0.9836498603650392, 0.9646976184832186, 0.9763846379992349, 0.9753762150801872, 0.9482335369948417, 0.8063292499986618, 0.1820743467738914, 0.0052021241935397535, 0.9662154895719975, 0.9404079431524904, 0.9378931204156763, 0.8714166948529851, 0.9354933040272786, 0.9967690919441977, 0.8910085879179537, 0.9882394894282943, 0.9300533154786702, 0.90364798873113, 0.9504218359672386, 0.9791374208056431, 0.9712551043375796, 0.12780489165194936, 0.7593114151086403, 0.10525108724278183, 0.9754092401647751, 0.909858452154963, 0.9522912911301367, 0.8486643744285708, 0.9254097395992237, 0.9546414203116578, 0.9398985700641797, 0.9084263712458351, 0.9662820595180672, 0.9075908888043874, 0.9405899739700454, 0.9640756972159614, 0.8439959581487829, 0.967809100926515, 0.9802797879689259, 0.9492905713660199, 0.19886069312279775, 0.7556706338666315, 0.9735506177398797, 0.9986921998793701, 0.9208084657214768, 0.8466052307986465, 0.7573851909070801, 0.22864458593421286, 0.9659101295285817, 0.9829371110314956, 0.9542782575163689, 0.882900897692648, 0.8736513386063302, 0.18516535310764642, 0.010892079594567438, 0.13615099493209296, 0.3213163480397394, 0.33765446743159055, 0.9318712673206762, 0.8766511494775955, 0.96979518896524, 0.9538337654458054, 0.9161486439882695, 0.8553232059548941, 0.8881802889348951, 0.10605137778327105, 0.867458173130227, 0.9959236368161718, 0.999302410629422, 0.9987322165353832, 0.9612036126627471, 0.9370370337749906, 0.9353342303316394, 0.059793055820287454, 0.8268738557889024, 0.027562461859630082, 0.15159354022796545, 0.965573269126655, 0.9551951718971441, 0.9633108759230667, 0.9731482168413134, 0.9782905448814972, 0.9727157074510558, 0.32402382106903915, 0.6739695478236015, 0.685722398531636, 0.1371444797063272, 0.1371444797063272, 0.9896735142455462, 0.9832011341669786, 0.9589445524619835, 0.9661689270426693, 0.9440363698743138, 0.9622014239800297, 0.9722581075914117, 0.9306553316050533, 0.9816493554649572, 0.954064774459955, 0.05805749554044716, 0.9289199286471546, 0.9667197191193224, 0.9182296245533147, 0.9990026194510715, 0.9764777139034585, 0.9610572021575954, 0.9807736122200491, 0.7699850775405206, 0.9775494373861885, 0.762958921781826, 0.9600207500252114, 0.9462239736185758, 0.9510047780877234, 0.9292249733738854, 0.056316665052962754, 0.9951348547785193, 0.998262236216795, 0.9620243146065949, 0.998170569187926, 0.9502890435618794, 0.8424568634385793, 0.9802493218279267, 0.7697431964455569, 0.20992996266697006, 0.9067391063865785, 0.9418561599221587, 0.9257015505199369, 0.8849633011656829, 0.9838443252853194, 0.5690267961681013, 0.38578087875803474, 0.038578087875803475, 0.7998031808043709, 0.9535709248371829, 0.9191350508619193, 0.9279697385739921, 0.8137491503612382, 0.9827967171073687, 0.8174310097759028, 0.9490087834053939, 0.9800136626398054, 0.9792608114106135, 0.9652903301379696, 0.99897282820638, 0.9490095913641189, 0.9854290670010631, 0.011872639361458593, 0.998391477198818, 0.8409719058786544, 0.9899706177386847, 0.9148861203914299, 0.8024019949759679, 0.19799529746160247, 0.9859422029418696, 0.010516716831379943, 0.939276561338964, 0.7972076225016402, 0.19223446216351608, 0.005653954769515178, 0.9532331098554035], \"Term\": [\"abomination\", \"absolute\", \"accent\", \"accept\", \"account\", \"act\", \"action\", \"advertisement\", \"aesthetic\", \"affect\", \"age\", \"ago\", \"also\", \"amazed\", \"amazing\", \"ancient\", \"annoy\", \"anticipation\", \"apart\", \"appearance\", \"appearance\", \"appreciation\", \"approach\", \"area\", \"art\", \"asleep\", \"astonish\", \"attempt\", \"attention\", \"attraction\", \"attractive\", \"automatically\", \"away\", \"away\", \"awful\", \"bad\", \"bad\", \"bamboo\", \"barely\", \"basically\", \"battle\", \"battle\", \"battle\", \"be\", \"beautifully\", \"beauty\", \"beforehand\", \"beginning\", \"believable\", \"blend\", \"blow\", \"body\", \"brand\", \"bridge\", \"brother\", \"budget\", \"cake\", \"camera\", \"camera\", \"case\", \"case\", \"catch\", \"character\", \"charisma\", \"charismatic\", \"child\", \"chill\", \"choose\", \"clearly\", \"clothe\", \"club\", \"combine\", \"comedic_relief\", \"comic\", \"common\", \"compel\", \"compelling\", \"comprehend\", \"consistently\", \"constantly\", \"constructive\", \"costume\", \"could\", \"covid\", \"creature\", \"creature\", \"crew\", \"dance\", \"date\", \"day\", \"debut\", \"decade\", \"deliver\", \"desire\", \"difficult\", \"directly\", \"disappointment\", \"display\", \"distinct\", \"double\", \"doubt\", \"dull\", \"effort\", \"emotion\", \"end\", \"engage\", \"enter\", \"entertaining\", \"entirely\", \"environment\", \"environment\", \"even\", \"even\", \"everywhere\", \"evil\", \"exaggerated\", \"exceptional\", \"exist\", \"explain\", \"exposition\", \"exposition\", \"extent\", \"extraordinary\", \"extremely\", \"failure\", \"fake\", \"familiar\", \"familiar\", \"fare\", \"fast\", \"father\", \"father\", \"fear\", \"feel\", \"female\", \"female\", \"fight\", \"film\", \"find\", \"find\", \"first\", \"fluidity\", \"form\", \"friend\", \"friend\", \"fully\", \"furthermore\", \"game\", \"genuinely\", \"get\", \"get\", \"ghost\", \"glue\", \"go\", \"go\", \"good\", \"great\", \"ground\", \"guess\", \"guy\", \"guy\", \"hall\", \"hand\", \"handle\", \"heavy\", \"help\", \"help\", \"hire\", \"history\", \"home\", \"human\", \"hurt\", \"immersion\", \"industry\", \"inevitably\", \"initially\", \"innovative\", \"inspiration\", \"iron_man\", \"journey\", \"kill\", \"know\", \"know\", \"lack\", \"lame\", \"landscape\", \"large\", \"leaf\", \"learn\", \"legend_ten\", \"legend_ten\", \"legend_ten\", \"legendary\", \"life\", \"life\", \"life\", \"list\", \"literally\", \"literally\", \"local\", \"logic\", \"lose\", \"lot\", \"major\", \"majority\", \"make\", \"marry\", \"marvel\", \"master\", \"masterpiece\", \"meet\", \"mess\", \"message\", \"mid_credit\", \"minor\", \"mixture\", \"monster\", \"monster\", \"mood\", \"mother\", \"motivation\", \"movement\", \"movie\", \"much\", \"multiple\", \"multiple\", \"must\", \"mystery\", \"mystic\", \"mystical\", \"myth\", \"mythical_creature\", \"mythological\", \"natural\", \"new\", \"night\", \"nowhere\", \"number\", \"number\", \"numerous\", \"obviously\", \"obviously\", \"often\", \"often\", \"one\", \"opinion\", \"organization\", \"otherwise\", \"otherworldly\", \"pandemic\", \"people\", \"people\", \"people\", \"person\", \"personality\", \"personally\", \"physical\", \"picture\", \"plot\", \"pointless\", \"poor\", \"poorly\", \"pop\", \"portion\", \"positive\", \"possible\", \"power\", \"power\", \"power\", \"praise\", \"premise\", \"previously\", \"pro\", \"profound\", \"proportion\", \"provide\", \"punch\", \"pure\", \"purely\", \"question\", \"racist\", \"random\", \"rarely\", \"rate\", \"razor_fist\", \"reach\", \"reach\", \"read\", \"really\", \"recall\", \"redemption\", \"reference\", \"reference\", \"relatable\", \"relate\", \"remarkable\", \"remove\", \"responsible\", \"ring\", \"ring\", \"ring\", \"ring\", \"ring\", \"rise\", \"rushed\", \"sad\", \"sadly\", \"sake\", \"satisfied\", \"satisfy\", \"satisfy\", \"saw\", \"say\", \"scene\", \"see\", \"seek\", \"sequel\", \"sequence\", \"sequence\", \"set\", \"set\", \"set\", \"sign\", \"signature\", \"significant\", \"similar\", \"simple\", \"single\", \"sister\", \"sister\", \"skill\", \"skill\", \"skill\", \"slattery\", \"slow\", \"smart\", \"son\", \"song\", \"sorry\", \"soul\", \"spectacle\", \"spectacular\", \"stellar\", \"step\", \"step\", \"stereotype\", \"stick\", \"story\", \"strange\", \"strike\", \"struggle\", \"substance\", \"subtitle\", \"suggest\", \"synergy\", \"tad\", \"territory\", \"thank\", \"thank\", \"thing\", \"think\", \"third_act\", \"time\", \"today\", \"ton\", \"totally\", \"train\", \"train\", \"trash\", \"tree\", \"tribute\", \"trope\", \"truth\", \"try\", \"try\", \"try\", \"ultimate\", \"understandably\", \"unexpected\", \"unfamiliar\", \"unfold\", \"unfortunately\", \"uninteresting\", \"unnecessary\", \"upset\", \"view\", \"visible\", \"watch\", \"watchable\", \"way\", \"way\", \"well\", \"will\", \"woman\", \"word\", \"world\", \"world\", \"would\", \"would\", \"wrap\", \"year\", \"year\", \"year\", \"young\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [6, 7, 3, 2, 5, 8, 9, 1, 4, 10]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el611400747160191529066011950\", ldavis_el611400747160191529066011950_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el611400747160191529066011950\", ldavis_el611400747160191529066011950_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el611400747160191529066011950\", ldavis_el611400747160191529066011950_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "visualize_topics(model, id2word, corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.\n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vATjQNTY8buA",
        "outputId": "eaaeb429-919e-4eb5-f9e7-4ee590033b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB scores:\n",
            "\tfit_time: 0.15\n",
            "\tscore_time: 0.04\n",
            "\ttest_accuracy: 0.72\n",
            "\ttrain_accuracy: 0.72\n",
            "\ttest_precision: 0.72\n",
            "\ttrain_precision: 0.72\n",
            "\ttest_recall: 0.72\n",
            "\ttrain_recall: 0.72\n",
            "\ttest_f1: 0.72\n",
            "\ttrain_f1: 0.72\n",
            "SGDClassifier scores:\n",
            "\tfit_time: 0.16\n",
            "\tscore_time: 0.04\n",
            "\ttest_accuracy: 0.80\n",
            "\ttrain_accuracy: 1.00\n",
            "\ttest_precision: 0.80\n",
            "\ttrain_precision: 1.00\n",
            "\ttest_recall: 0.80\n",
            "\ttrain_recall: 1.00\n",
            "\ttest_f1: 0.80\n",
            "\ttrain_f1: 1.00\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Extract features and targets from dataframe\n",
        "X, y = df['review'], df['sentiment']\n",
        "\n",
        "# Create pipelines for different models\n",
        "nb_clf = Pipeline([('count_vectorizer', CountVectorizer()),\n",
        "                   ('tfidf_transformer', TfidfTransformer()),\n",
        "                   ('nb', MultinomialNB())])\n",
        "sgdc_clf = Pipeline([('count_vectorizer', CountVectorizer()),\n",
        "                     ('tfidf_transformer', TfidfTransformer()),\n",
        "                     ('nb', SGDClassifier())])\n",
        "\n",
        "# Perform cross validation over the models\n",
        "scoring = {\n",
        "    'accuracy': metrics.make_scorer(metrics.accuracy_score),\n",
        "    'precision': metrics.make_scorer(metrics.precision_score, average='micro'),\n",
        "    'recall': metrics.make_scorer(metrics.recall_score, average='micro'),\n",
        "    'f1': metrics.make_scorer(metrics.f1_score, average='micro')\n",
        "}\n",
        "nb_scores = cross_validate(nb_clf,\n",
        "                           X,\n",
        "                           y,\n",
        "                           cv=5,\n",
        "                           scoring=scoring,\n",
        "                           return_train_score=True,\n",
        "                           n_jobs=-1)\n",
        "sgdc_scores = cross_validate(sgdc_clf,\n",
        "                             X,\n",
        "                             y,\n",
        "                             cv=5,\n",
        "                             scoring=scoring,\n",
        "                             return_train_score=True,\n",
        "                             n_jobs=-1)\n",
        "\n",
        "\n",
        "def print_scores(model_name, scores):\n",
        "    print(f'{model_name} scores:')\n",
        "    for score, values in scores.items():\n",
        "        print(f'\\t{score}: {np.mean(values):.2f}')\n",
        "\n",
        "\n",
        "print_scores('MultinomialNB', nb_scores)\n",
        "print_scores('SGDClassifier', sgdc_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download here: https://github.com/unt-iialab/info5731-spring2022/blob/main/assignments/assignment4-question3-data.zip. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j3AvoKNa78L",
        "outputId": "9db87a4f-48fa-40ae-af40-f79933a58d54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train R2 Score: 0.9825035547268258\n"
          ]
        }
      ],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\n",
        "\n",
        "# Load training and test data\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "# Preprocess the data\n",
        "Xy_train = df_train.dropna(axis=1)\n",
        "X_train, y_train = Xy_train.drop(['SalePrice'], axis=1), Xy_train['SalePrice']\n",
        "X_test = df_test[X_train.columns]\n",
        "\n",
        "# Fill null values in the test dataset \n",
        "fill_mode = lambda col: col.fillna(col.mode()[0])\n",
        "X_test = X_test.apply(fill_mode, axis=0)\n",
        "\n",
        "# Select categorical columns for encoding \n",
        "categorical_cols = X_train.select_dtypes(exclude=['number']).columns\n",
        "\n",
        "# Prepare pipeline\n",
        "reg_pipeline = Pipeline([\n",
        "    ('transformer',\n",
        "     ColumnTransformer([\n",
        "         ('encoder', OrdinalEncoder(), categorical_cols)\n",
        "     ],\n",
        "                       remainder='passthrough')),\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('reg', RandomForestRegressor()),\n",
        "])\n",
        "\n",
        "# Train the regression model\n",
        "reg_pipeline.fit(X_train, y_train)\n",
        "y_train_pred = reg_pipeline.predict(X_train)\n",
        "\n",
        "train_score = metrics.r2_score(y_train, y_train_pred)\n",
        "print(f'Train R2 Score: {train_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7VQg5FVsa78L"
      },
      "outputs": [],
      "source": [
        "# Predict and save predictions for the test data\n",
        "y_test_pred = reg_pipeline.predict(X_test)\n",
        "y_test_pred = pd.Series(y_test_pred)\n",
        "y_test_pred.to_csv('test_predictions.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "INFO5731_Assignment_Four-1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "282cfe75ed14aad7ebe1f5efe0d715ce0bf6dd7cea934ae0f2e79e1bedd9f3e1"
    },
    "kernelspec": {
      "display_name": "Python 3.10.1 ('datascience')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}